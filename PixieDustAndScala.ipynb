{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZ4b0rQ9DqOnQteH/fud17",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjtalkar/DP-203-Azure-Data-Engineering-Notes/blob/main/PixieDustAndScala.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQUoZMZcHYDa"
      },
      "source": [
        "#https://pixiedust.github.io/pixiedust/install.html\n",
        "#PixeDustAndScala.ipynb\n",
        "#https://github.com/pixiedust/pixiedust/blob/master/install/createKernel.py\n",
        "#https://archive.apache.org/dist/spark/\n",
        "\n",
        "# Step 1: PIXIEDUST_HOME: /root/pixiedust\n",
        "# \tKeep y/n [y]? n\n",
        "# Please enter a PIXIEDUST_HOME location: /content/pixiedust\n",
        "# Step 2: SPARK_HOME: /content/spark-3.2.0-bin-hadoop3.2\n",
        "# \tKeep y/n [y]? n\n",
        "# Step 2: Please enter a SPARK_HOME location: /content/spark-2.3.2\n",
        "# Directory /content/spark-2.3.2 does not exist\n",
        "# \tCreate y/n [y]? y\n",
        "# What version would you like to download? 1.6.3, 2.0.2, 2.1.0, 2.2.0, 2.3.2 [2.3.2]: \n",
        "# SPARK_HOME will be set to /content/spark-2.3.2/spark-2.3.2-bin-hadoop2.7\n",
        "# Downloading Spark 2.3.2\n",
        "\n",
        "### Successful installation\n",
        "# Step 1: PIXIEDUST_HOME: /root/pixiedust\n",
        "# \tKeep y/n [y]? y\n",
        "# Step 2: SPARK_HOME: /content/spark-2.3.2-bin-hadoop2.7\n",
        "# \tKeep y/n [y]? y\n",
        "#  22 %\n",
        "#  45 %\n",
        "#  68 %\n",
        "#  91 %\n",
        "#  100 %\n",
        "      \n",
        "# downloaded spark cloudant jar: /root/pixiedust/bin/cloudant-spark-v2.0.0-185.jar\n",
        "# Step 3: SCALA_HOME: /root/pixiedust/bin/scala\n",
        "# \tKeep y/n [y]? y\n",
        "# Directory /root/pixiedust/bin/scala does not contain a valid scala install\n",
        "# \tDownload Scala y/n [y]? y\n",
        "# SCALA_HOME will be set to /root/pixiedust/bin/scala/scala-2.11.8\n",
        "# Downloading Scala 2.11"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1DPQHkmKGoT"
      },
      "source": [
        "#Since pixiedust will need a spark home, install and set spark home first. Mount the google drive so that the downlaoded version of spark that works with delta lake is the one installed.\n",
        "#Spark and delta lake are finicky in the sense not all spark and delta lake versions work well together and the errors resulting from the mismatch are not easy to decipher from the error messages\n",
        "#Now pixiedust only works with these spark versions: What version would you like to download? 1.6.3, 2.0.2, 2.1.0, 2.2.0, 2.3.2 [2.3.2]:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmoXNo0wJ1Co",
        "outputId": "55d20274-a892-41ed-f8a8-89c650b6f7e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UzL0UzFJ86_",
        "outputId": "e5599d6e-40b2-4b41-ea7c-5dc278191257"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!tar xf 'gdrive/My Drive/Databricks/spark-2.3.2-bin-hadoop2.7.tgz' \n",
        "!pip -q install findspark\n",
        "!pip install ipython-sql"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.7/dist-packages (0.3.9)\n",
            "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (2.2.1)\n",
            "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (5.5.0)\n",
            "Requirement already satisfied: sqlalchemy>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.4.25)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.15.0)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.4.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (5.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=1.0->ipython-sql) (0.2.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.7.4.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=1.0->ipython-sql) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYdKsegmKBF7"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.7.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPYCPu89Iga_",
        "outputId": "a9c21d98-f6b1-46fc-ffbf-b585d2d44307"
      },
      "source": [
        "!pip install pixiedust"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pixiedust in /usr/local/lib/python3.7/dist-packages (1.1.19)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pixiedust) (2.23.0)\n",
            "Requirement already satisfied: geojson in /usr/local/lib/python3.7/dist-packages (from pixiedust) (2.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pixiedust) (1.1.5)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from pixiedust) (1.6.3)\n",
            "Requirement already satisfied: colour in /usr/local/lib/python3.7/dist-packages (from pixiedust) (0.1.5)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from pixiedust) (3.3.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pixiedust) (3.2.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->pixiedust) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->pixiedust) (0.37.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown->pixiedust) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown->pixiedust) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown->pixiedust) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pixiedust) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pixiedust) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pixiedust) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pixiedust) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pixiedust) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pixiedust) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pixiedust) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pixiedust) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pixiedust) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pixiedust) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nojt4IRMOqxQ",
        "outputId": "563d043e-74c8-4fea-b6ea-a1bdf5e330fd"
      },
      "source": [
        "!jupyter pixiedust install"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1mStep 1: PIXIEDUST_HOME: /root/pixiedust\u001b[0m\n",
            "\tKeep y/n [y]? y\n",
            "\u001b[32;1mStep 2: SPARK_HOME: /content/spark-2.3.2-bin-hadoop2.7\u001b[0m\n",
            "\tKeep y/n [y]? y\n",
            " 22 %\n",
            "\u001b[F 45 %\n",
            "\u001b[F 68 %\n",
            "\u001b[F 91 %\n",
            "\u001b[F 100 %\n",
            "\u001b[F      \n",
            "\u001b[Fdownloaded spark cloudant jar: /root/pixiedust/bin/cloudant-spark-v2.0.0-185.jar\n",
            "\u001b[32;1mStep 3: SCALA_HOME: /root/pixiedust/bin/scala\u001b[0m\n",
            "\tKeep y/n [y]? y\n",
            "\u001b[32;1mDirectory /root/pixiedust/bin/scala does not contain a valid scala install\u001b[0m\n",
            "\tDownload Scala y/n [y]? y\n",
            "SCALA_HOME will be set to /root/pixiedust/bin/scala/scala-2.11.8\n",
            "Downloading Scala 2.11\n",
            " 3 %\n",
            "\u001b[F 7 %\n",
            "\u001b[F 10 %\n",
            "\u001b[F 14 %\n",
            "\u001b[F 18 %\n",
            "\u001b[F 21 %\n",
            "\u001b[F 25 %\n",
            "\u001b[F 29 %\n",
            "\u001b[F 32 %\n",
            "\u001b[F 36 %\n",
            "\u001b[F 40 %\n",
            "\u001b[F 43 %\n",
            "\u001b[F 47 %\n",
            "\u001b[F 51 %\n",
            "\u001b[F 54 %\n",
            "\u001b[F 58 %\n",
            "\u001b[F 62 %\n",
            "\u001b[F 65 %\n",
            "\u001b[F 69 %\n",
            "\u001b[F 73 %\n",
            "\u001b[F 76 %\n",
            "\u001b[F 80 %\n",
            "\u001b[F 84 %\n",
            "\u001b[F 87 %\n",
            "\u001b[F 91 %\n",
            "\u001b[F 95 %\n",
            "\u001b[F 98 %\n",
            "\u001b[F 100 %\n",
            "\u001b[F      \n",
            "\u001b[FExtracting Scala 2.11 to /root/pixiedust/bin/scala\n",
            " 0 %\n",
            "\u001b[F 1 %\n",
            "\u001b[F 3 %\n",
            "\u001b[F 5 %\n",
            "\u001b[F 6 %\n",
            "\u001b[F 8 %\n",
            "\u001b[F 10 %\n",
            "\u001b[F 11 %\n",
            "\u001b[F 13 %\n",
            "\u001b[F 15 %\n",
            "\u001b[F 16 %\n",
            "\u001b[F 18 %\n",
            "\u001b[F 20 %\n",
            "\u001b[F 22 %\n",
            "\u001b[F 23 %\n",
            "\u001b[F 25 %\n",
            "\u001b[F 27 %\n",
            "\u001b[F 28 %\n",
            "\u001b[F 30 %\n",
            "\u001b[F 32 %\n",
            "\u001b[F 33 %\n",
            "\u001b[F 35 %\n",
            "\u001b[F 37 %\n",
            "\u001b[F 38 %\n",
            "\u001b[F 40 %\n",
            "\u001b[F 42 %\n",
            "\u001b[F 44 %\n",
            "\u001b[F 45 %\n",
            "\u001b[F 47 %\n",
            "\u001b[F 49 %\n",
            "\u001b[F 50 %\n",
            "\u001b[F 52 %\n",
            "\u001b[F 54 %\n",
            "\u001b[F 55 %\n",
            "\u001b[F 57 %\n",
            "\u001b[F 59 %\n",
            "\u001b[F 61 %\n",
            "\u001b[F 62 %\n",
            "\u001b[F 64 %\n",
            "\u001b[F 66 %\n",
            "\u001b[F 67 %\n",
            "\u001b[F 69 %\n",
            "\u001b[F 71 %\n",
            "\u001b[F 72 %\n",
            "\u001b[F 74 %\n",
            "\u001b[F 76 %\n",
            "\u001b[F 77 %\n",
            "\u001b[F 79 %\n",
            "\u001b[F 81 %\n",
            "\u001b[F 83 %\n",
            "\u001b[F 84 %\n",
            "\u001b[F 86 %\n",
            "\u001b[F 88 %\n",
            "\u001b[F 89 %\n",
            "\u001b[F 91 %\n",
            "\u001b[F 93 %\n",
            "\u001b[F 94 %\n",
            "\u001b[F 96 %\n",
            "\u001b[F 98 %\n",
            "\u001b[F      \n",
            "\u001b[F\u001b[32;1mStep 4: Kernel Name: Python-with-Pixiedust_Spark-2.3\u001b[0m\n",
            "\tKeep y/n [y]? y\n",
            "self.kernelInternalName pythonwithpixiedustspark23\n",
            "[PixiedustInstall] Installed kernelspec pythonwithpixiedustspark23 in /root/.local/share/jupyter/kernels/pythonwithpixiedustspark23\n",
            "Downloading intro notebooks into /root/pixiedust/notebooks\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 1 - Easy Visualizations.ipynb\n",
            " 100 %\n",
            "\u001b[F      \n",
            "\u001b[F\u001b[F\u001b[F\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 1 - Easy Visualizations.ipynb : \u001b[32;1mdone\u001b[0m\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 2 - Working with External Data.ipynb\n",
            " 100 %\n",
            "\u001b[F      \n",
            "\u001b[F\u001b[F\u001b[F\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 2 - Working with External Data.ipynb : \u001b[32;1mdone\u001b[0m\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 3 - Scala and Python.ipynb\n",
            " 100 %\n",
            "\u001b[F      \n",
            "\u001b[F\u001b[F\u001b[F\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 3 - Scala and Python.ipynb : \u001b[32;1mdone\u001b[0m\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 4 - Add External Spark Packages.ipynb\n",
            " 100 %\n",
            "\u001b[F      \n",
            "\u001b[F\u001b[F\u001b[F\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 4 - Add External Spark Packages.ipynb : \u001b[32;1mdone\u001b[0m\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 5 - Stash to Cloudant.ipynb\n",
            " 100 %\n",
            "\u001b[F      \n",
            "\u001b[F\u001b[F\u001b[F\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust 5 - Stash to Cloudant.ipynb : \u001b[32;1mdone\u001b[0m\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust Contribute.ipynb\n",
            " 100 %\n",
            "\u001b[F      \n",
            "\u001b[F\u001b[F\u001b[F\n",
            "...https://github.com/ibm-watson-data-lab/pixiedust/raw/master/notebook/PixieDust Contribute.ipynb : \u001b[32;1mdone\u001b[0m\n",
            "\n",
            "\n",
            "####################################################################################################\n",
            "#\tCongratulations: Kernel Python-with-Pixiedust_Spark-2.3 was successfully created in /root/.local/share/jupyter/kernels/pythonwithpixiedustspark23\n",
            "#\tYou can start the Notebook server with the following command:\n",
            "#\t\t\u001b[32;1mjupyter notebook /root/pixiedust/notebooks\u001b[0m\n",
            "####################################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JUDuVpqSgnX",
        "outputId": "51b84d6a-9d6a-4596-da3a-114ca5476c5c"
      },
      "source": [
        "!jupyter pixiedust list\n",
        "\n",
        "#pythonwithpixiedustspark23    /root/.local/share/jupyter/kernels/pythonwithpixiedustspark23\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available kernels:\n",
            "  pythonwithpixiedustspark23    /root/.local/share/jupyter/kernels/pythonwithpixiedustspark23\n",
            "  ir                            /usr/local/share/jupyter/kernels/ir\n",
            "  python2                       /usr/local/share/jupyter/kernels/python2\n",
            "  python3                       /usr/local/share/jupyter/kernels/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXRxwcI5S5W4",
        "outputId": "76ef831a-f43b-4347-aff2-a3ecd3d6b21a"
      },
      "source": [
        "%%scala\n",
        "\n",
        "import java.time.{LocalDate}\n",
        "import java.time.format.DateTimeFormatter\n",
        "import java.time.temporal.ChronoUnit\n",
        "\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.functions._\n",
        "import scala.collection.mutable\n",
        "\n",
        "def calculateLastday(wrappedArray: mutable.WrappedArray[Row]) :String ={\n",
        "  val orderedList = wrappedArray.sortWith((x:Row, y:Row) => {\n",
        "    if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "      x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "    } else {\n",
        "      \n",
        "      val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "        x.getAs[String](\"payment_plan_days\") +\n",
        "        x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "      val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "        y.getAs[String](\"payment_plan_days\") +\n",
        "        y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "      //same plan, always subscribe then unsubscribe\n",
        "      if(x_sig != y_sig) {\n",
        "        x_sig > y_sig\n",
        "      } else {\n",
        "        if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "          //multiple cancel, consecutive cancels should only put the expiration date earlier\n",
        "          x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "        } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "          //multiple renewal, expiration date keeps extending\n",
        "          x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "        } else {\n",
        "          //same day same plan transaction: subscription preceeds cancellation\n",
        "          x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  })\n",
        "  orderedList.last.getAs[String](\"membership_expire_date\")\n",
        "}\n",
        "\n",
        "def calculateRenewalGap(log:mutable.WrappedArray[Row], lastExpiration: String): Int = {\n",
        "  val orderedDates = log.sortWith((x:Row, y:Row) => {\n",
        "    if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "      x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "    } else {\n",
        "      \n",
        "      val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "        x.getAs[String](\"payment_plan_days\") +\n",
        "        x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "      val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "        y.getAs[String](\"payment_plan_days\") +\n",
        "        y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "      //same data same plan transaction, assumption: subscribe then unsubscribe\n",
        "      if(x_sig != y_sig) {\n",
        "        x_sig > y_sig\n",
        "      } else {\n",
        "        if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "          //multiple cancel of same plan, consecutive cancels should only put the expiration date earlier\n",
        "          x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "        } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "          //multiple renewal, expire date keep extending\n",
        "          x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "        } else {\n",
        "          //same date cancel should follow subscription\n",
        "          x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  })\n",
        "\n",
        "  //Search for the first subscription after expiration\n",
        "  //If active cancel is the first action, find the gap between the cancellation and renewal\n",
        "  val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
        "  var lastExpireDate = LocalDate.parse(s\"${lastExpiration.substring(0,4)}-${lastExpiration.substring(4,6)}-${lastExpiration.substring(6,8)}\", formatter)\n",
        "  var gap = 9999\n",
        "  for(\n",
        "    date <- orderedDates\n",
        "    if gap == 9999\n",
        "  ) {\n",
        "    val transString = date.getAs[String](\"transaction_date\")\n",
        "    val transDate = LocalDate.parse(s\"${transString.substring(0,4)}-${transString.substring(4,6)}-${transString.substring(6,8)}\", formatter)\n",
        "    val expireString = date.getAs[String](\"membership_expire_date\")\n",
        "    val expireDate = LocalDate.parse(s\"${expireString.substring(0,4)}-${expireString.substring(4,6)}-${expireString.substring(6,8)}\", formatter)\n",
        "    val isCancel = date.getAs[String](\"is_cancel\")\n",
        "\n",
        "    if(isCancel == \"1\") {\n",
        "      if(expireDate.isBefore(lastExpireDate)) {\n",
        "        lastExpireDate = expireDate\n",
        "      }\n",
        "    } else {\n",
        "      gap = ChronoUnit.DAYS.between(lastExpireDate, transDate).toInt\n",
        "    }\n",
        "  }\n",
        "  gap\n",
        "}\n",
        "\n",
        "val data = spark\n",
        "  .read\n",
        "  .option(\"header\", value = true)\n",
        "  .csv(\"/mnt/kkbox/transactions/\")\n",
        "\n",
        "val historyCutoff = \"20170131\"\n",
        "\n",
        "val historyData = data.filter(col(\"transaction_date\")>=\"20170101\" and col(\"transaction_date\")<=lit(historyCutoff))\n",
        "val futureData = data.filter(col(\"transaction_date\") > lit(historyCutoff))\n",
        "\n",
        "val calculateLastdayUDF = udf(calculateLastday _)\n",
        "val userExpire = historyData\n",
        "  .groupBy(\"msno\")\n",
        "  .agg(\n",
        "    calculateLastdayUDF(\n",
        "      collect_list(\n",
        "        struct(\n",
        "          col(\"payment_method_id\"),\n",
        "          col(\"payment_plan_days\"),\n",
        "          col(\"plan_list_price\"),\n",
        "          col(\"transaction_date\"),\n",
        "          col(\"membership_expire_date\"),\n",
        "          col(\"is_cancel\")\n",
        "        )\n",
        "      )\n",
        "    ).alias(\"last_expire\")\n",
        "  )\n",
        "\n",
        "val predictionCandidates = userExpire\n",
        "  .filter(\n",
        "    col(\"last_expire\") >= \"20170201\" and col(\"last_expire\") <= \"20170228\"\n",
        "  )\n",
        "  .select(\"msno\", \"last_expire\")\n",
        "\n",
        "\n",
        "val joinedData = predictionCandidates\n",
        "  .join(futureData,Seq(\"msno\"), \"left_outer\")\n",
        "\n",
        "val noActivity = joinedData\n",
        "  .filter(col(\"payment_method_id\").isNull)\n",
        "  .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "\n",
        "val calculateRenewalGapUDF = udf(calculateRenewalGap _)\n",
        "val renewals = joinedData\n",
        "  .filter(col(\"payment_method_id\").isNotNull)\n",
        "  .groupBy(\"msno\", \"last_expire\")\n",
        "  .agg(\n",
        "    calculateRenewalGapUDF(\n",
        "      collect_list(\n",
        "        struct(\n",
        "          col(\"payment_method_id\"),\n",
        "          col(\"payment_plan_days\"),\n",
        "          col(\"plan_list_price\"),\n",
        "          col(\"transaction_date\"),\n",
        "          col(\"membership_expire_date\"),\n",
        "          col(\"is_cancel\")\n",
        "        )\n",
        "      ),\n",
        "      col(\"last_expire\")\n",
        "    ).alias(\"gap\")\n",
        "  )\n",
        "\n",
        "val validRenewals = renewals.filter(col(\"gap\") < 30)\n",
        "  .withColumn(\"is_churn\", lit(0))\n",
        "val lateRenewals = renewals.filter(col(\"gap\") >= 30)\n",
        "  .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "val resultSet = validRenewals\n",
        "  .select(\"msno\",\"is_churn\")\n",
        "  .union(\n",
        "    lateRenewals\n",
        "      .select(\"msno\",\"is_churn\")\n",
        "      .union(\n",
        "        noActivity.select(\"msno\",\"is_churn\")\n",
        "      )\n",
        "  )\n",
        "\n",
        "resultSet.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/kkbox/silver/train/\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%scala` not found.\n"
          ]
        }
      ]
    }
  ]
}