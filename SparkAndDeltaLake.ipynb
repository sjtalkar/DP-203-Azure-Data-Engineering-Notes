{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6rb+8fOki2A25hVhZLGUi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjtalkar/DP-203-Azure-Data-Engineering-Notes/blob/main/SparkAndDeltaLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjFVFoiSGZiL",
        "outputId": "b7bde3c9-91d5-4ba8-f3fa-4804d9f44618"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-rilfzZimYV"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "949yQj6ltEoW"
      },
      "source": [
        "# %%shell\n",
        "# SCALA_VERSION=2.12.8 ALMOND_VERSION=0.3.0+16-548dc10f-SNAPSHOT\n",
        "# curl -Lo coursier https://git.io/coursier-cli\n",
        "# chmod +x coursier\n",
        "# ./coursier bootstrap \\\n",
        "#     -r jitpack -r sonatype:snapshots \\\n",
        "#     -i user -I user:sh.almond:scala-kernel-api_$SCALA_VERSION:$ALMOND_VERSION \\\n",
        "#     sh.almond:scala-kernel_$SCALA_VERSION:$ALMOND_VERSION \\\n",
        "#     --sources --default=true \\\n",
        "#     -o almond-snapshot --embed-files=false\n",
        "# rm coursier\n",
        "# ./almond-snapshot --install --global --force\n",
        "# rm almond-snapshot"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVGzxQS2i6MS"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kVqkjNBU6K6"
      },
      "source": [
        "# %%shell\n",
        "# echo \"{\n",
        "#   \\\"language\\\" : \\\"scala\\\",\n",
        "#   \\\"display_name\\\" : \\\"Scala\\\",\n",
        "#   \\\"argv\\\" : [\n",
        "#     \\\"bash\\\",\n",
        "#     \\\"-c\\\",\n",
        "#     \\\"env LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libpython3.6m.so:\\$LD_PRELOAD java -jar /usr/local/share/jupyter/kernels/scala/launcher.jar --connection-file {connection_file}\\\"\n",
        "#   ]\n",
        "# }\" > /usr/local/share/jupyter/kernels/scala/kernel.json"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XXkSqkEH6Rb",
        "outputId": "d26f7883-c457-4a74-d5d1-c8d692a288e1"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!tar xf 'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz' \n",
        "!pip -q install findspark\n",
        "!pip install ipython-sql"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.7/dist-packages (0.3.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: sqlalchemy>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.4.25)\n",
            "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (5.5.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (2.2.1)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (5.1.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=1.0->ipython-sql) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (4.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.6.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=1.0->ipython-sql) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IDFUblaIUAp",
        "outputId": "1e90976e-591b-49cd-d8ca-75a7d3414ebe"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-apxLALIXQL",
        "outputId": "f213415e-ec51-427c-a86f-d43c0598e7a3"
      },
      "source": [
        "%ls 'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxykmwXDI9H6",
        "outputId": "7d224234-51c8-4291-db7d-b1ea876c8491"
      },
      "source": [
        "%ls /content"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mspark-3.0.0-bin-hadoop3.2\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ_Pm5r-JL0q"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.7.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HmDC0BtJPNn"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWujwd9IJQWm"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder.appName('delta_session').getOrCreate()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dpJl9WlJW1U"
      },
      "source": [
        "from pyspark.sql.types import *"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di9dwcFbcsI6"
      },
      "source": [
        "# create database to house SQL tables\n",
        "_ = spark.sql('CREATE DATABASE IF NOT EXISTS kkbox')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B2IN33SJd7N"
      },
      "source": [
        "transaction_schema = StructType([\n",
        "            StructField('msno', StringType()),\n",
        "            StructField('payment_method_id', IntegerType()),\n",
        "            StructField('payment_plan_days', IntegerType()),\n",
        "            StructField('plan_list_price', IntegerType()),\n",
        "            StructField('actual_amount_paid', IntegerType()),\n",
        "            StructField('is_auto_renew', IntegerType()),\n",
        "            StructField('transaction_date', DateType()),\n",
        "            StructField('membership_expire_date', DateType()),\n",
        "            StructField('is_cancel', IntegerType()),\n",
        "            ])\n",
        "#read data from csv\n",
        "transactions = (\n",
        "                spark\n",
        "                  .read\n",
        "                  .csv(\n",
        "                      'gdrive/My Drive/Databricks/transactions.csv',\n",
        "                       schema=transaction_schema,\n",
        "                       header=True,\n",
        "                       dateFormat = 'yyyyMMdd'\n",
        "                  )\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOTK_NBNTaXB"
      },
      "source": [
        "# Work with the data using DELTA"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-m1HTpsJo28"
      },
      "source": [
        "#persist in delta lake format - essentially a parquet file with additional features such as history and versioning\n",
        "# This creates  folders called  G:\\My Drive\\Databricks\\transactions\\transaction_date=2015-01-01 ......\n",
        "(\n",
        "  transactions\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .partitionBy('transaction_date')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/transactions')    \n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_hKMEXMdYk8"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrBLeRUuesWB",
        "outputId": "0a20f64c-9780-4ab4-b2f1-3618b2824729"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "    DROP  TABLE  IF EXISTS kkbox.transactions\n",
        "\"\"\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkc125ybRbJh",
        "outputId": "d784bd0d-053b-4b7a-db99-8775430b18d7"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE  kkbox.transactions \n",
        "    USING DELTA\n",
        "    LOCATION 'gdrive/My Drive/Databricks/kkbox/transactions'\n",
        "\"\"\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkzjnOzLULkz"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.transactions LIMIT 10\n",
        "''')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ8Urc8FcVVd",
        "outputId": "d7bbdfdb-3809-4e39-df97-b7c77681fea1"
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "|                msno|payment_method_id|payment_plan_days|plan_list_price|actual_amount_paid|is_auto_renew|transaction_date|membership_expire_date|is_cancel|\n",
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "|+2HVGotjiE2ofWgVp...|               41|               30|             99|                99|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+5Yo2rxxC+x+kIYl4...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+BForXQeVUWKHbTM/...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+L/XrtIg0DD9ku+ik...|               33|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+NFlZlsTdfUWAxDiE...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+OQ8X0rnnrPOUvtAB...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+SzJCXTBfFx9+tps9...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-12|        0|\n",
            "|+ZLD2EVyD7TQs3gUw...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+bnLGdIZW5uxgyUA2...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+hE9XWSj68wdPEaE8...|               36|               30|            180|               180|            1|      2017-03-31|            2017-04-30|        0|\n",
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfsYOip4U1xL"
      },
      "source": [
        "\n",
        "# members dataset schema\n",
        "member_schema = StructType([\n",
        "  StructField('msno', StringType()),\n",
        "  StructField('city', IntegerType()),\n",
        "  StructField('bd', IntegerType()),\n",
        "  StructField('gender', StringType()),\n",
        "  StructField('registered_via', IntegerType()),\n",
        "  StructField('registration_init_time', DateType())\n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "members = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'gdrive/My Drive/Databricks/members.csv',\n",
        "      schema=member_schema,\n",
        "      header=True,\n",
        "      dateFormat='yyyyMMdd'\n",
        "      )\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdoDbzWgYsmr"
      },
      "source": [
        "# persist in delta lake format\n",
        "(\n",
        "  members\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/members')\n",
        "  )\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM2FUTE7YtvB"
      },
      "source": [
        "\n",
        "  # create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "    CREATE TABLE kkbox.members \n",
        "    USING DELTA \n",
        "    LOCATION 'gdrive/My Drive/Databricks/kkbox/members'\n",
        "    ''')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNLrTKO0VlAs"
      },
      "source": [
        "\n",
        "#Load User Logs Table\n",
        "\n",
        "_ = spark.sql('DROP TABLE IF EXISTS kkbox.user_logs')\n",
        "\n",
        "# drop any old delta lake files that might have been created\n",
        "#shutil.rmtree('gdrive/My Drive/Databricks/kkbox/user_logs', ignore_errors=True)\n",
        "\n",
        "# user logs dataset schema\n",
        "user_logs_schema = StructType([ \n",
        "  StructField('msno', StringType()),\n",
        "  StructField('date', DateType()),\n",
        "  StructField('num_25', IntegerType()),\n",
        "  StructField('num_50', IntegerType()),\n",
        "  StructField('num_75', IntegerType()),\n",
        "  StructField('num_985', IntegerType()),\n",
        "  StructField('num_100', IntegerType()),\n",
        "  StructField('num_uniq', IntegerType()),\n",
        "  StructField('total_secs', FloatType())  \n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "user_logs = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'gdrive/My Drive/Databricks/user_logs.csv',\n",
        "      schema=user_logs_schema,\n",
        "      header=True,\n",
        "      dateFormat='yyyyMMdd'\n",
        "      )\n",
        "    )\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c09ghGwVZi80"
      },
      "source": [
        "\n",
        "# persist in delta lake format\n",
        "( user_logs\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .partitionBy('date')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/user_logs')\n",
        "  )"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUPz3g7RZl0h"
      },
      "source": [
        "\n",
        "\n",
        "# create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE IF NOT EXISTS kkbox.user_logs\n",
        "  USING DELTA \n",
        "  LOCATION 'gdrive/My Drive/Databricks/kkbox/user_logs'\n",
        "  ''')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFWgyddPbDRC"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.members LIMIT 10\n",
        "''')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPSSy4GjbtAq",
        "outputId": "60f35384-a1da-41c6-fc78-ada205edc2d4"
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "|                msno|city| bd|gender|registered_via|registration_init_time|\n",
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "|Rb9UwLQTrxzBVwCB6...|   1|  0|  null|            11|            2011-09-11|\n",
            "|+tJonkh+O1CA796Fm...|   1|  0|  null|             7|            2011-09-14|\n",
            "|cV358ssn7a0f7jZOw...|   1|  0|  null|            11|            2011-09-15|\n",
            "|9bzDeJP6sQodK73K5...|   1|  0|  null|            11|            2011-09-15|\n",
            "|WFLY3s7z4EZsieHCt...|   6| 32|female|             9|            2011-09-15|\n",
            "|yLkV2gbZ4GLFwqTOX...|   4| 30|  male|             9|            2011-09-16|\n",
            "|jNCGK78YkTyId3H3w...|   1|  0|  null|             7|            2011-09-16|\n",
            "|WH5Jq4mgtfUFXh2yz...|   5| 34|  male|             9|            2011-09-16|\n",
            "|tKmbR4X5VXjHmxERr...|   5| 19|  male|             9|            2011-09-17|\n",
            "|I0yFvqMoNkM8ZNHb6...|  13| 63|  male|             9|            2011-09-18|\n",
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_DJq3ljdq-y"
      },
      "source": [
        "# Step 2: Acquire Churn Labels\n",
        "# To build our model, we will need to identify which customers have churned within two periods of interest.\n",
        "# These periods are February 2017 and March 2017. We will train our model to predict churn in February 2017 and then evaluate our model's ability \n",
        "# to predict churn in March 2017, making these our training and testing datasets, respectively.\n",
        "\n",
        "# Per instructions provided in the Kaggle competition, a KKBox subscriber is not identified as churned until he or she fails to renew their \n",
        "#subscription 30-days following its expiration. Most subscriptions are themselves on a 30-day renewal schedule (though some subscriptions renew on significantly longer cycles). This means that identifying churn involves a sequential walk through the customer data, looking for renewal gaps that would indicate a customer churned on a prior expiration date.\n",
        "\n",
        "# While the competition makes available pre-labeled training and testing datasets, train.csv and train_v2.csv, respectively, several past \n",
        "#participants have noted that these datasets should be regenerated. A Scala script for doing so is provided by KKBox. Modifying the script for this environment, we might regenerate our training and test datasets as follows:"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwz43Q0t3H9"
      },
      "source": [
        "# %scala\n",
        "\n",
        "# import java.time.{LocalDate}\n",
        "# import java.time.format.DateTimeFormatter\n",
        "# import java.time.temporal.ChronoUnit\n",
        "\n",
        "# import org.apache.spark.sql.{Row, SparkSession}\n",
        "# import org.apache.spark.sql.functions._\n",
        "# import scala.collection.mutable\n",
        "\n",
        "# def calculateLastday(wrappedArray: mutable.WrappedArray[Row]) :String ={\n",
        "#   val orderedList = wrappedArray.sortWith((x:Row, y:Row) => {\n",
        "#     if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "#       x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "#     } else {\n",
        "      \n",
        "#       val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "#         x.getAs[String](\"payment_plan_days\") +\n",
        "#         x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "#         y.getAs[String](\"payment_plan_days\") +\n",
        "#         y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       //same plan, always subscribe then unsubscribe\n",
        "#       if(x_sig != y_sig) {\n",
        "#         x_sig > y_sig\n",
        "#       } else {\n",
        "#         if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "#           //multiple cancel, consecutive cancels should only put the expiration date earlier\n",
        "#           x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "#         } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "#           //multiple renewal, expiration date keeps extending\n",
        "#           x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "#         } else {\n",
        "#           //same day same plan transaction: subscription preceeds cancellation\n",
        "#           x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "#         }\n",
        "#       }\n",
        "#     }\n",
        "#   })\n",
        "#   orderedList.last.getAs[String](\"membership_expire_date\")\n",
        "# }\n",
        "\n",
        "# def calculateRenewalGap(log:mutable.WrappedArray[Row], lastExpiration: String): Int = {\n",
        "#   val orderedDates = log.sortWith((x:Row, y:Row) => {\n",
        "#     if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "#       x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "#     } else {\n",
        "      \n",
        "#       val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "#         x.getAs[String](\"payment_plan_days\") +\n",
        "#         x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "#         y.getAs[String](\"payment_plan_days\") +\n",
        "#         y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       //same data same plan transaction, assumption: subscribe then unsubscribe\n",
        "#       if(x_sig != y_sig) {\n",
        "#         x_sig > y_sig\n",
        "#       } else {\n",
        "#         if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "#           //multiple cancel of same plan, consecutive cancels should only put the expiration date earlier\n",
        "#           x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "#         } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "#           //multiple renewal, expire date keep extending\n",
        "#           x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "#         } else {\n",
        "#           //same date cancel should follow subscription\n",
        "#           x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "#         }\n",
        "#       }\n",
        "#     }\n",
        "#   })\n",
        "\n",
        "#   //Search for the first subscription after expiration\n",
        "#   //If active cancel is the first action, find the gap between the cancellation and renewal\n",
        "#   val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
        "#   var lastExpireDate = LocalDate.parse(s\"${lastExpiration.substring(0,4)}-${lastExpiration.substring(4,6)}-${lastExpiration.substring(6,8)}\", formatter)\n",
        "#   var gap = 9999\n",
        "#   for(\n",
        "#     date <- orderedDates\n",
        "#     if gap == 9999\n",
        "#   ) {\n",
        "#     val transString = date.getAs[String](\"transaction_date\")\n",
        "#     val transDate = LocalDate.parse(s\"${transString.substring(0,4)}-${transString.substring(4,6)}-${transString.substring(6,8)}\", formatter)\n",
        "#     val expireString = date.getAs[String](\"membership_expire_date\")\n",
        "#     val expireDate = LocalDate.parse(s\"${expireString.substring(0,4)}-${expireString.substring(4,6)}-${expireString.substring(6,8)}\", formatter)\n",
        "#     val isCancel = date.getAs[String](\"is_cancel\")\n",
        "\n",
        "#     if(isCancel == \"1\") {\n",
        "#       if(expireDate.isBefore(lastExpireDate)) {\n",
        "#         lastExpireDate = expireDate\n",
        "#       }\n",
        "#     } else {\n",
        "#       gap = ChronoUnit.DAYS.between(lastExpireDate, transDate).toInt\n",
        "#     }\n",
        "#   }\n",
        "#   gap\n",
        "# }\n",
        "\n",
        "# val data = spark\n",
        "#   .read\n",
        "#   .option(\"header\", value = true)\n",
        "#   .csv(\"/mnt/kkbox/transactions/\")\n",
        "\n",
        "# val historyCutoff = \"20170131\"\n",
        "\n",
        "# val historyData = data.filter(col(\"transaction_date\")>=\"20170101\" and col(\"transaction_date\")<=lit(historyCutoff))\n",
        "# val futureData = data.filter(col(\"transaction_date\") > lit(historyCutoff))\n",
        "\n",
        "# val calculateLastdayUDF = udf(calculateLastday _)\n",
        "# val userExpire = historyData\n",
        "#   .groupBy(\"msno\")\n",
        "#   .agg(\n",
        "#     calculateLastdayUDF(\n",
        "#       collect_list(\n",
        "#         struct(\n",
        "#           col(\"payment_method_id\"),\n",
        "#           col(\"payment_plan_days\"),\n",
        "#           col(\"plan_list_price\"),\n",
        "#           col(\"transaction_date\"),\n",
        "#           col(\"membership_expire_date\"),\n",
        "#           col(\"is_cancel\")\n",
        "#         )\n",
        "#       )\n",
        "#     ).alias(\"last_expire\")\n",
        "#   )\n",
        "\n",
        "# val predictionCandidates = userExpire\n",
        "#   .filter(\n",
        "#     col(\"last_expire\") >= \"20170201\" and col(\"last_expire\") <= \"20170228\"\n",
        "#   )\n",
        "#   .select(\"msno\", \"last_expire\")\n",
        "\n",
        "\n",
        "# val joinedData = predictionCandidates\n",
        "#   .join(futureData,Seq(\"msno\"), \"left_outer\")\n",
        "\n",
        "# val noActivity = joinedData\n",
        "#   .filter(col(\"payment_method_id\").isNull)\n",
        "#   .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "\n",
        "# val calculateRenewalGapUDF = udf(calculateRenewalGap _)\n",
        "# val renewals = joinedData\n",
        "#   .filter(col(\"payment_method_id\").isNotNull)\n",
        "#   .groupBy(\"msno\", \"last_expire\")\n",
        "#   .agg(\n",
        "#     calculateRenewalGapUDF(\n",
        "#       collect_list(\n",
        "#         struct(\n",
        "#           col(\"payment_method_id\"),\n",
        "#           col(\"payment_plan_days\"),\n",
        "#           col(\"plan_list_price\"),\n",
        "#           col(\"transaction_date\"),\n",
        "#           col(\"membership_expire_date\"),\n",
        "#           col(\"is_cancel\")\n",
        "#         )\n",
        "#       ),\n",
        "#       col(\"last_expire\")\n",
        "#     ).alias(\"gap\")\n",
        "#   )\n",
        "\n",
        "# val validRenewals = renewals.filter(col(\"gap\") < 30)\n",
        "#   .withColumn(\"is_churn\", lit(0))\n",
        "# val lateRenewals = renewals.filter(col(\"gap\") >= 30)\n",
        "#   .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "# val resultSet = validRenewals\n",
        "#   .select(\"msno\",\"is_churn\")\n",
        "#   .union(\n",
        "#     lateRenewals\n",
        "#       .select(\"msno\",\"is_churn\")\n",
        "#       .union(\n",
        "#         noActivity.select(\"msno\",\"is_churn\")\n",
        "#       )\n",
        "#   )\n",
        "\n",
        "# resultSet.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/kkbox/silver/train/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}