{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLGtMNk4uLer/ihfrr3stP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjtalkar/DP-203-Azure-Data-Engineering-Notes/blob/main/SparkAndDeltaLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjFVFoiSGZiL",
        "outputId": "4f851945-cfa9-450a-b4cc-8b8048aa7ee4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-rilfzZimYV"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "949yQj6ltEoW"
      },
      "source": [
        "# %%shell\n",
        "# SCALA_VERSION=2.12.8 ALMOND_VERSION=0.3.0+16-548dc10f-SNAPSHOT\n",
        "# curl -Lo coursier https://git.io/coursier-cli\n",
        "# chmod +x coursier\n",
        "# ./coursier bootstrap \\\n",
        "#     -r jitpack -r sonatype:snapshots \\\n",
        "#     -i user -I user:sh.almond:scala-kernel-api_$SCALA_VERSION:$ALMOND_VERSION \\\n",
        "#     sh.almond:scala-kernel_$SCALA_VERSION:$ALMOND_VERSION \\\n",
        "#     --sources --default=true \\\n",
        "#     -o almond-snapshot --embed-files=false\n",
        "# rm coursier\n",
        "# ./almond-snapshot --install --global --force\n",
        "# rm almond-snapshot"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVGzxQS2i6MS"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kVqkjNBU6K6"
      },
      "source": [
        "# %%shell\n",
        "# echo \"{\n",
        "#   \\\"language\\\" : \\\"scala\\\",\n",
        "#   \\\"display_name\\\" : \\\"Scala\\\",\n",
        "#   \\\"argv\\\" : [\n",
        "#     \\\"bash\\\",\n",
        "#     \\\"-c\\\",\n",
        "#     \\\"env LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libpython3.6m.so:\\$LD_PRELOAD java -jar /usr/local/share/jupyter/kernels/scala/launcher.jar --connection-file {connection_file}\\\"\n",
        "#   ]\n",
        "# }\" > /usr/local/share/jupyter/kernels/scala/kernel.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4VU_SxlDXbl"
      },
      "source": [
        "# Spark's version 3.0.0 is being used here so that Delta Lake Core works well with it. \n",
        "\n",
        "#### Versions are something that will plague you when you have to work with Spark and other libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XXkSqkEH6Rb",
        "outputId": "abfdc63b-e569-47fe-c591-a6a3d05b9127"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!tar xf 'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz' \n",
        "!pip -q install findspark\n",
        "!pip install ipython-sql"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.7/dist-packages (0.3.9)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.4.2)\n",
            "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.15.0)\n",
            "Requirement already satisfied: sqlalchemy>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.4.25)\n",
            "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (5.5.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (2.2.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (5.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=1.0->ipython-sql) (0.2.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.7.4.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=1.0->ipython-sql) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IDFUblaIUAp",
        "outputId": "08f66507-129d-4010-925e-c1bcd08c9a24"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-apxLALIXQL",
        "outputId": "94116034-6e6a-4ebf-df91-f18b5a0fec55"
      },
      "source": [
        "%ls 'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxykmwXDI9H6",
        "outputId": "9bcb52e2-520f-471c-cff5-7e394ccba6fa"
      },
      "source": [
        "%ls /content"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mspark-3.0.0-bin-hadoop3.2\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ_Pm5r-JL0q"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.7.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HmDC0BtJPNn"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWujwd9IJQWm"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder.appName('delta_session').getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dpJl9WlJW1U"
      },
      "source": [
        "from pyspark.sql.types import *"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di9dwcFbcsI6"
      },
      "source": [
        "# create database to house SQL tables\n",
        "_ = spark.sql('CREATE DATABASE IF NOT EXISTS kkbox')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B2IN33SJd7N"
      },
      "source": [
        "transaction_schema = StructType([\n",
        "            StructField('msno', StringType()),\n",
        "            StructField('payment_method_id', IntegerType()),\n",
        "            StructField('payment_plan_days', IntegerType()),\n",
        "            StructField('plan_list_price', IntegerType()),\n",
        "            StructField('actual_amount_paid', IntegerType()),\n",
        "            StructField('is_auto_renew', IntegerType()),\n",
        "            StructField('transaction_date', DateType()),\n",
        "            StructField('membership_expire_date', DateType()),\n",
        "            StructField('is_cancel', IntegerType()),\n",
        "            ])\n",
        "#read data from csv\n",
        "transactions = (\n",
        "                spark\n",
        "                  .read\n",
        "                  .csv(\n",
        "                      'gdrive/My Drive/Databricks/transactions.csv',\n",
        "                       schema=transaction_schema,\n",
        "                       header=True,\n",
        "                       dateFormat = 'yyyyMMdd'\n",
        "                  )\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obdLNRb7ESHD"
      },
      "source": [
        "# Work with the data using DELTA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-m1HTpsJo28"
      },
      "source": [
        "#persist in delta lake format - essentially a parquet file with additional features such as history and versioning\n",
        "# This creates  folders called  G:\\My Drive\\Databricks\\transactions\\transaction_date=2015-01-01 ......\n",
        "(\n",
        "  transactions\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .partitionBy('transaction_date')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/transactions')    \n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrBLeRUuesWB",
        "outputId": "6e846cf0-588e-49a5-dd3c-f6669171511d"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "    DROP  TABLE  IF EXISTS kkbox.transactions\n",
        "\"\"\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkc125ybRbJh",
        "outputId": "a374912e-60fc-4a0d-d282-d59ca563b2b0"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE  kkbox.transactions \n",
        "    USING DELTA\n",
        "    LOCATION 'gdrive/My Drive/Databricks/kkbox/transactions'\n",
        "\"\"\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkzjnOzLULkz"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.transactions LIMIT 10\n",
        "''')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ8Urc8FcVVd",
        "outputId": "a424a887-074c-44dc-9523-65e6d2064a5f"
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "|                msno|payment_method_id|payment_plan_days|plan_list_price|actual_amount_paid|is_auto_renew|transaction_date|membership_expire_date|is_cancel|\n",
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "|+2HVGotjiE2ofWgVp...|               41|               30|             99|                99|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+5Yo2rxxC+x+kIYl4...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+BForXQeVUWKHbTM/...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+L/XrtIg0DD9ku+ik...|               33|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+NFlZlsTdfUWAxDiE...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+OQ8X0rnnrPOUvtAB...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+SzJCXTBfFx9+tps9...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-12|        0|\n",
            "|+ZLD2EVyD7TQs3gUw...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+bnLGdIZW5uxgyUA2...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+hE9XWSj68wdPEaE8...|               36|               30|            180|               180|            1|      2017-03-31|            2017-04-30|        0|\n",
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfsYOip4U1xL"
      },
      "source": [
        "\n",
        "# members dataset schema\n",
        "member_schema = StructType([\n",
        "  StructField('msno', StringType()),\n",
        "  StructField('city', IntegerType()),\n",
        "  StructField('bd', IntegerType()),\n",
        "  StructField('gender', StringType()),\n",
        "  StructField('registered_via', IntegerType()),\n",
        "  StructField('registration_init_time', DateType())\n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "members = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'gdrive/My Drive/Databricks/members.csv',\n",
        "      schema=member_schema,\n",
        "      header=True,\n",
        "      dateFormat='yyyyMMdd'\n",
        "      )\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdoDbzWgYsmr"
      },
      "source": [
        "# persist in delta lake format\n",
        "(\n",
        "  members\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/members')\n",
        "  )\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM2FUTE7YtvB"
      },
      "source": [
        "\n",
        "  # create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "    CREATE TABLE kkbox.members \n",
        "    USING DELTA \n",
        "    LOCATION 'gdrive/My Drive/Databricks/kkbox/members'\n",
        "    ''')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNLrTKO0VlAs"
      },
      "source": [
        "\n",
        "#Load User Logs Table\n",
        "\n",
        "_ = spark.sql('DROP TABLE IF EXISTS kkbox.user_logs')\n",
        "\n",
        "# drop any old delta lake files that might have been created\n",
        "#shutil.rmtree('gdrive/My Drive/Databricks/kkbox/user_logs', ignore_errors=True)\n",
        "\n",
        "# user logs dataset schema\n",
        "user_logs_schema = StructType([ \n",
        "  StructField('msno', StringType()),\n",
        "  StructField('date', DateType()),\n",
        "  StructField('num_25', IntegerType()),\n",
        "  StructField('num_50', IntegerType()),\n",
        "  StructField('num_75', IntegerType()),\n",
        "  StructField('num_985', IntegerType()),\n",
        "  StructField('num_100', IntegerType()),\n",
        "  StructField('num_uniq', IntegerType()),\n",
        "  StructField('total_secs', FloatType())  \n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "user_logs = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'gdrive/My Drive/Databricks/user_logs.csv',\n",
        "      schema=user_logs_schema,\n",
        "      header=True,\n",
        "      dateFormat='yyyyMMdd'\n",
        "      )\n",
        "    )\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c09ghGwVZi80"
      },
      "source": [
        "\n",
        "# persist in delta lake format\n",
        "( user_logs\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .partitionBy('date')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/user_logs')\n",
        "  )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUPz3g7RZl0h"
      },
      "source": [
        "\n",
        "\n",
        "# create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE IF NOT EXISTS kkbox.user_logs\n",
        "  USING DELTA \n",
        "  LOCATION 'gdrive/My Drive/Databricks/kkbox/user_logs'\n",
        "  ''')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFWgyddPbDRC"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.members LIMIT 10\n",
        "''')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPSSy4GjbtAq",
        "outputId": "a748d73f-1a35-43f5-996b-a0fbc748a4fb"
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "|                msno|city| bd|gender|registered_via|registration_init_time|\n",
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "|Rb9UwLQTrxzBVwCB6...|   1|  0|  null|            11|            2011-09-11|\n",
            "|+tJonkh+O1CA796Fm...|   1|  0|  null|             7|            2011-09-14|\n",
            "|cV358ssn7a0f7jZOw...|   1|  0|  null|            11|            2011-09-15|\n",
            "|9bzDeJP6sQodK73K5...|   1|  0|  null|            11|            2011-09-15|\n",
            "|WFLY3s7z4EZsieHCt...|   6| 32|female|             9|            2011-09-15|\n",
            "|yLkV2gbZ4GLFwqTOX...|   4| 30|  male|             9|            2011-09-16|\n",
            "|jNCGK78YkTyId3H3w...|   1|  0|  null|             7|            2011-09-16|\n",
            "|WH5Jq4mgtfUFXh2yz...|   5| 34|  male|             9|            2011-09-16|\n",
            "|tKmbR4X5VXjHmxERr...|   5| 19|  male|             9|            2011-09-17|\n",
            "|I0yFvqMoNkM8ZNHb6...|  13| 63|  male|             9|            2011-09-18|\n",
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEjakLwaFTMp"
      },
      "source": [
        "# Get Training Data from CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W3qofZtE_ir"
      },
      "source": [
        "train_schema = StructType([\n",
        "            StructField('msno', StringType()),\n",
        "            StructField('is_churn', IntegerType()),\n",
        "            ])\n",
        "#read data from csv\n",
        "train = (\n",
        "                spark\n",
        "                  .read\n",
        "                  .csv(\n",
        "                      'gdrive/My Drive/Databricks/train_v2.csv',\n",
        "                       schema=train_schema,\n",
        "                       header=True\n",
        "                  )\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-JcIrkkG5MV"
      },
      "source": [
        "(\n",
        "  train\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/train')    \n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL3vQFPYPEy5"
      },
      "source": [
        "\n",
        "# create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE IF NOT EXISTS kkbox.train\n",
        "  USING DELTA \n",
        "  LOCATION 'gdrive/My Drive/Databricks/kkbox/train'\n",
        "  ''')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0PFTdVkPen-"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.train LIMIT 10\n",
        "''')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0WBIvJKPabS",
        "outputId": "a420433b-beb1-4b8c-a98f-e238544e028a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+\n",
            "|                msno|is_churn|\n",
            "+--------------------+--------+\n",
            "|ugx0CjOMzazClkFzU...|       1|\n",
            "|f/NmvEzHfhINFEYZT...|       1|\n",
            "|zLo9f73nGGT1p21lt...|       1|\n",
            "|8iF/+8HY8lJKFrTc7...|       1|\n",
            "|K6fja4+jmoZ5xG6By...|       1|\n",
            "|ibIHVYBqxGwrSExE6...|       1|\n",
            "|kVmM8X4iBPCOfK/m1...|       1|\n",
            "|moRTKhKIDvb+C8ZHO...|       1|\n",
            "|dW/tPZMDh2Oz/ksdu...|       1|\n",
            "|otEcMhAX3mU4gumUS...|       1|\n",
            "+--------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8zi4I_0Pl7O",
        "outputId": "08f6d420-a9f9-4849-f613-b945addf43d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(msno='ugx0CjOMzazClkFzU2xasmDZaoIqOUAZPsH1q0teWCg=', is_churn=1),\n",
              " Row(msno='f/NmvEzHfhINFEYZTR05prUdr+E+3+oewvweYz9cCQE=', is_churn=1),\n",
              " Row(msno='zLo9f73nGGT1p21ltZC3ChiRnAVvgibMyazbCxvWPcg=', is_churn=1),\n",
              " Row(msno='8iF/+8HY8lJKFrTc7iR9ZYGCG2Ecrogbc2Vy5YhsfhQ=', is_churn=1),\n",
              " Row(msno='K6fja4+jmoZ5xG6BypqX80Uw/XKpMgrEMdG2edFOxnA=', is_churn=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs6DBNA5QOGA"
      },
      "source": [
        "# Set up SQL magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tc3btEzQVVc",
        "outputId": "412d1f35-3465-451f-cb9c-06333fd5584c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        }
      },
      "source": [
        "!pip install sparksql-magic"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparksql-magic\n",
            "  Downloading sparksql_magic-0.0.3-py36-none-any.whl (4.3 kB)\n",
            "Collecting pyspark>=2.3.0\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 33 kB/s \n",
            "\u001b[?25hCollecting ipython>=7.4.0\n",
            "  Downloading ipython-7.29.0-py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.1.3)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.22-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.18.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (5.1.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.4.0->sparksql-magic) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.4.0->sparksql-magic) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.4.0->sparksql-magic) (0.2.5)\n",
            "Collecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=3e8af06fb1b31a3210d4df7d63c5ddd54ec5cfd41d3a63476021c07f2a8a198c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, prompt-toolkit, pyspark, ipython, sparksql-magic\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.22 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.29.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.29.0 prompt-toolkit-3.0.22 py4j-0.10.9.2 pyspark-3.2.0 sparksql-magic-0.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit",
                  "py4j",
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kesdkjq5RiQz"
      },
      "source": [
        "%load_ext sparksql_magic\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MbIBmKIR__S",
        "outputId": "911807f1-d785-498c-ca21-568c7a5796a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%config SparkSql.max_num_rows = 20"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Config option `max_num_rows` not recognized by `SparkSql`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7NzJYLNS13H",
        "outputId": "658a1018-506f-4dd6-89c1-d1cc24c11768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "%%sparksql\n",
        "select * from kkbox.transactions limit 20"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">msno</td><td style=\"font-weight: bold\">payment_method_id</td><td style=\"font-weight: bold\">payment_plan_days</td><td style=\"font-weight: bold\">plan_list_price</td><td style=\"font-weight: bold\">actual_amount_paid</td><td style=\"font-weight: bold\">is_auto_renew</td><td style=\"font-weight: bold\">transaction_date</td><td style=\"font-weight: bold\">membership_expire_date</td><td style=\"font-weight: bold\">is_cancel</td></tr><tr><td>+2HVGotjiE2ofWgVp37vLKGzJgBiUkdKRiBlf/Wji90=</td><td>41</td><td>30</td><td>99</td><td>99</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+5Yo2rxxC+x+kIYl4to0D1GTrKacKcaGG0sV38NLIn4=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+BForXQeVUWKHbTM/nXSRGfhhzcWmfNfvz+HbE77Pjc=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+L/XrtIg0DD9ku+ik959EtrnRQfLyf/P6GD1Z8HbkP4=</td><td>33</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+NFlZlsTdfUWAxDiEZ08D/ux0o+uY3ps1+6wy4I9ybg=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+OQ8X0rnnrPOUvtABzE+MRE9zqSjTzSrI42qawMQ2uU=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+SzJCXTBfFx9+tps9joOuArGrOsGoCD7Utj9aon7X+M=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-12</td><td>0</td></tr><tr><td>+ZLD2EVyD7TQs3gUwTUeWDce5ZxUOSNpAXFd0roNO9o=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+bnLGdIZW5uxgyUA26TXwDMnTHTuDlbj4mFEAQtmmDo=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+hE9XWSj68wdPEaE8nyb4RN7i2xRRcN0VG13OUEJ4PE=</td><td>36</td><td>30</td><td>180</td><td>180</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+xhvvfof24+SVClyP4n9YqWwvCmtj+ODhW14gOlUldc=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+yY4OSgYRiBwxCk/9jyfRkvwnjY8zpaC9TBgIpx1WFk=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-15</td><td>0</td></tr><tr><td>/3T/9RetFt4UGlip/yfz8W8IR1PsleTWMMqfDRniYtA=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-11</td><td>0</td></tr><tr><td>/Avu5fPm7P+I1VeASu4RrQHS7+aJ15MAfEs9gXR+OnI=</td><td>33</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>/BndJt9YSOh1kzEykXyHrQZKl943rqFrzR9efW2b7wE=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-24</td><td>0</td></tr><tr><td>/CQ/QEofoZl1RDyTxTiiqEPkyndOB8jmrVcfPWl3fSM=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-12</td><td>0</td></tr><tr><td>/JM8FrTRFo1Y7Q1m9s7y+xxonqKFH2q2XzeISo9AYys=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-27</td><td>0</td></tr><tr><td>/KugT/YLwkcVVjiY3TFPA7h5lRBcpXTYmzeSsKDpj1U=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-01</td><td>0</td></tr><tr><td>/M85NXODpGq7PPQM7QcsI4OzGryJO+E/YoKCbW5f5VE=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-25</td><td>0</td></tr><tr><td>/Nz6zkgxuH21icQt19QXmTBQZ+4RdN8omN6Iijbuh5M=</td><td>41</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTyjsN-6FC_E"
      },
      "source": [
        "# Using Scala For labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_DJq3ljdq-y"
      },
      "source": [
        "# Step 2: Acquire Churn Labels\n",
        "# To build our model, we will need to identify which customers have churned within two periods of interest.\n",
        "# These periods are February 2017 and March 2017. We will train our model to predict churn in February 2017 and then evaluate our model's ability \n",
        "# to predict churn in March 2017, making these our training and testing datasets, respectively.\n",
        "\n",
        "# Per instructions provided in the Kaggle competition, a KKBox subscriber is not identified as churned until he or she fails to renew their \n",
        "#subscription 30-days following its expiration. Most subscriptions are themselves on a 30-day renewal schedule (though some subscriptions renew on significantly longer cycles). This means that identifying churn involves a sequential walk through the customer data, looking for renewal gaps that would indicate a customer churned on a prior expiration date.\n",
        "\n",
        "# While the competition makes available pre-labeled training and testing datasets, train.csv and train_v2.csv, respectively, several past \n",
        "#participants have noted that these datasets should be regenerated. A Scala script for doing so is provided by KKBox. Modifying the script for this environment, we might regenerate our training and test datasets as follows:"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwz43Q0t3H9"
      },
      "source": [
        "# %scala\n",
        "\n",
        "# import java.time.{LocalDate}\n",
        "# import java.time.format.DateTimeFormatter\n",
        "# import java.time.temporal.ChronoUnit\n",
        "\n",
        "# import org.apache.spark.sql.{Row, SparkSession}\n",
        "# import org.apache.spark.sql.functions._\n",
        "# import scala.collection.mutable\n",
        "\n",
        "# def calculateLastday(wrappedArray: mutable.WrappedArray[Row]) :String ={\n",
        "#   val orderedList = wrappedArray.sortWith((x:Row, y:Row) => {\n",
        "#     if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "#       x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "#     } else {\n",
        "      \n",
        "#       val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "#         x.getAs[String](\"payment_plan_days\") +\n",
        "#         x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "#         y.getAs[String](\"payment_plan_days\") +\n",
        "#         y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       //same plan, always subscribe then unsubscribe\n",
        "#       if(x_sig != y_sig) {\n",
        "#         x_sig > y_sig\n",
        "#       } else {\n",
        "#         if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "#           //multiple cancel, consecutive cancels should only put the expiration date earlier\n",
        "#           x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "#         } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "#           //multiple renewal, expiration date keeps extending\n",
        "#           x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "#         } else {\n",
        "#           //same day same plan transaction: subscription preceeds cancellation\n",
        "#           x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "#         }\n",
        "#       }\n",
        "#     }\n",
        "#   })\n",
        "#   orderedList.last.getAs[String](\"membership_expire_date\")\n",
        "# }\n",
        "\n",
        "# def calculateRenewalGap(log:mutable.WrappedArray[Row], lastExpiration: String): Int = {\n",
        "#   val orderedDates = log.sortWith((x:Row, y:Row) => {\n",
        "#     if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "#       x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "#     } else {\n",
        "      \n",
        "#       val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "#         x.getAs[String](\"payment_plan_days\") +\n",
        "#         x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "#         y.getAs[String](\"payment_plan_days\") +\n",
        "#         y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       //same data same plan transaction, assumption: subscribe then unsubscribe\n",
        "#       if(x_sig != y_sig) {\n",
        "#         x_sig > y_sig\n",
        "#       } else {\n",
        "#         if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "#           //multiple cancel of same plan, consecutive cancels should only put the expiration date earlier\n",
        "#           x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "#         } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "#           //multiple renewal, expire date keep extending\n",
        "#           x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "#         } else {\n",
        "#           //same date cancel should follow subscription\n",
        "#           x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "#         }\n",
        "#       }\n",
        "#     }\n",
        "#   })\n",
        "\n",
        "#   //Search for the first subscription after expiration\n",
        "#   //If active cancel is the first action, find the gap between the cancellation and renewal\n",
        "#   val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
        "#   var lastExpireDate = LocalDate.parse(s\"${lastExpiration.substring(0,4)}-${lastExpiration.substring(4,6)}-${lastExpiration.substring(6,8)}\", formatter)\n",
        "#   var gap = 9999\n",
        "#   for(\n",
        "#     date <- orderedDates\n",
        "#     if gap == 9999\n",
        "#   ) {\n",
        "#     val transString = date.getAs[String](\"transaction_date\")\n",
        "#     val transDate = LocalDate.parse(s\"${transString.substring(0,4)}-${transString.substring(4,6)}-${transString.substring(6,8)}\", formatter)\n",
        "#     val expireString = date.getAs[String](\"membership_expire_date\")\n",
        "#     val expireDate = LocalDate.parse(s\"${expireString.substring(0,4)}-${expireString.substring(4,6)}-${expireString.substring(6,8)}\", formatter)\n",
        "#     val isCancel = date.getAs[String](\"is_cancel\")\n",
        "\n",
        "#     if(isCancel == \"1\") {\n",
        "#       if(expireDate.isBefore(lastExpireDate)) {\n",
        "#         lastExpireDate = expireDate\n",
        "#       }\n",
        "#     } else {\n",
        "#       gap = ChronoUnit.DAYS.between(lastExpireDate, transDate).toInt\n",
        "#     }\n",
        "#   }\n",
        "#   gap\n",
        "# }\n",
        "\n",
        "# val data = spark\n",
        "#   .read\n",
        "#   .option(\"header\", value = true)\n",
        "#   .csv(\"/mnt/kkbox/transactions/\")\n",
        "\n",
        "# val historyCutoff = \"20170131\"\n",
        "\n",
        "# val historyData = data.filter(col(\"transaction_date\")>=\"20170101\" and col(\"transaction_date\")<=lit(historyCutoff))\n",
        "# val futureData = data.filter(col(\"transaction_date\") > lit(historyCutoff))\n",
        "\n",
        "# val calculateLastdayUDF = udf(calculateLastday _)\n",
        "# val userExpire = historyData\n",
        "#   .groupBy(\"msno\")\n",
        "#   .agg(\n",
        "#     calculateLastdayUDF(\n",
        "#       collect_list(\n",
        "#         struct(\n",
        "#           col(\"payment_method_id\"),\n",
        "#           col(\"payment_plan_days\"),\n",
        "#           col(\"plan_list_price\"),\n",
        "#           col(\"transaction_date\"),\n",
        "#           col(\"membership_expire_date\"),\n",
        "#           col(\"is_cancel\")\n",
        "#         )\n",
        "#       )\n",
        "#     ).alias(\"last_expire\")\n",
        "#   )\n",
        "\n",
        "# val predictionCandidates = userExpire\n",
        "#   .filter(\n",
        "#     col(\"last_expire\") >= \"20170201\" and col(\"last_expire\") <= \"20170228\"\n",
        "#   )\n",
        "#   .select(\"msno\", \"last_expire\")\n",
        "\n",
        "\n",
        "# val joinedData = predictionCandidates\n",
        "#   .join(futureData,Seq(\"msno\"), \"left_outer\")\n",
        "\n",
        "# val noActivity = joinedData\n",
        "#   .filter(col(\"payment_method_id\").isNull)\n",
        "#   .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "\n",
        "# val calculateRenewalGapUDF = udf(calculateRenewalGap _)\n",
        "# val renewals = joinedData\n",
        "#   .filter(col(\"payment_method_id\").isNotNull)\n",
        "#   .groupBy(\"msno\", \"last_expire\")\n",
        "#   .agg(\n",
        "#     calculateRenewalGapUDF(\n",
        "#       collect_list(\n",
        "#         struct(\n",
        "#           col(\"payment_method_id\"),\n",
        "#           col(\"payment_plan_days\"),\n",
        "#           col(\"plan_list_price\"),\n",
        "#           col(\"transaction_date\"),\n",
        "#           col(\"membership_expire_date\"),\n",
        "#           col(\"is_cancel\")\n",
        "#         )\n",
        "#       ),\n",
        "#       col(\"last_expire\")\n",
        "#     ).alias(\"gap\")\n",
        "#   )\n",
        "\n",
        "# val validRenewals = renewals.filter(col(\"gap\") < 30)\n",
        "#   .withColumn(\"is_churn\", lit(0))\n",
        "# val lateRenewals = renewals.filter(col(\"gap\") >= 30)\n",
        "#   .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "# val resultSet = validRenewals\n",
        "#   .select(\"msno\",\"is_churn\")\n",
        "#   .union(\n",
        "#     lateRenewals\n",
        "#       .select(\"msno\",\"is_churn\")\n",
        "#       .union(\n",
        "#         noActivity.select(\"msno\",\"is_churn\")\n",
        "#       )\n",
        "#   )\n",
        "\n",
        "# resultSet.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/kkbox/silver/train/\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5E0GEgexe5x"
      },
      "source": [
        "#https://pixiedust.github.io/pixiedust/install.html"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}