{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP28J0jtrsVPB9i3XPA79rG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjtalkar/DP-203-Azure-Data-Engineering-Notes/blob/main/SparkAndDeltaLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjFVFoiSGZiL",
        "outputId": "4f851945-cfa9-450a-b4cc-8b8048aa7ee4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-rilfzZimYV"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "949yQj6ltEoW"
      },
      "source": [
        "# %%shell\n",
        "# SCALA_VERSION=2.12.8 ALMOND_VERSION=0.3.0+16-548dc10f-SNAPSHOT\n",
        "# curl -Lo coursier https://git.io/coursier-cli\n",
        "# chmod +x coursier\n",
        "# ./coursier bootstrap \\\n",
        "#     -r jitpack -r sonatype:snapshots \\\n",
        "#     -i user -I user:sh.almond:scala-kernel-api_$SCALA_VERSION:$ALMOND_VERSION \\\n",
        "#     sh.almond:scala-kernel_$SCALA_VERSION:$ALMOND_VERSION \\\n",
        "#     --sources --default=true \\\n",
        "#     -o almond-snapshot --embed-files=false\n",
        "# rm coursier\n",
        "# ./almond-snapshot --install --global --force\n",
        "# rm almond-snapshot"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVGzxQS2i6MS"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kVqkjNBU6K6"
      },
      "source": [
        "# %%shell\n",
        "# echo \"{\n",
        "#   \\\"language\\\" : \\\"scala\\\",\n",
        "#   \\\"display_name\\\" : \\\"Scala\\\",\n",
        "#   \\\"argv\\\" : [\n",
        "#     \\\"bash\\\",\n",
        "#     \\\"-c\\\",\n",
        "#     \\\"env LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libpython3.6m.so:\\$LD_PRELOAD java -jar /usr/local/share/jupyter/kernels/scala/launcher.jar --connection-file {connection_file}\\\"\n",
        "#   ]\n",
        "# }\" > /usr/local/share/jupyter/kernels/scala/kernel.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4VU_SxlDXbl"
      },
      "source": [
        "# Spark's version 3.0.0 is being used here so that Delta Lake Core works well with it. \n",
        "\n",
        "#### Versions are something that will plague you when you have to work with Spark and other libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XXkSqkEH6Rb",
        "outputId": "abfdc63b-e569-47fe-c591-a6a3d05b9127"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!tar xf 'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz' \n",
        "!pip -q install findspark\n",
        "!pip install ipython-sql"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.7/dist-packages (0.3.9)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.4.2)\n",
            "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.15.0)\n",
            "Requirement already satisfied: sqlalchemy>=0.6.7 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.4.25)\n",
            "Requirement already satisfied: ipython>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (5.5.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (2.2.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (5.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=1.0->ipython-sql) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=1.0->ipython-sql) (0.2.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=0.6.7->ipython-sql) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=0.6.7->ipython-sql) (3.7.4.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=1.0->ipython-sql) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IDFUblaIUAp",
        "outputId": "08f66507-129d-4010-925e-c1bcd08c9a24"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-apxLALIXQL",
        "outputId": "94116034-6e6a-4ebf-df91-f18b5a0fec55"
      },
      "source": [
        "%ls 'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'gdrive/My Drive/Databricks/spark-3.0.0-bin-hadoop3.2.tgz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxykmwXDI9H6",
        "outputId": "9bcb52e2-520f-471c-cff5-7e394ccba6fa"
      },
      "source": [
        "%ls /content"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mspark-3.0.0-bin-hadoop3.2\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ_Pm5r-JL0q"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.7.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HmDC0BtJPNn"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWujwd9IJQWm"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder.appName('delta_session').getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dpJl9WlJW1U"
      },
      "source": [
        "from pyspark.sql.types import *"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di9dwcFbcsI6"
      },
      "source": [
        "# create database to house SQL tables\n",
        "_ = spark.sql('CREATE DATABASE IF NOT EXISTS kkbox')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B2IN33SJd7N"
      },
      "source": [
        "transaction_schema = StructType([\n",
        "            StructField('msno', StringType()),\n",
        "            StructField('payment_method_id', IntegerType()),\n",
        "            StructField('payment_plan_days', IntegerType()),\n",
        "            StructField('plan_list_price', IntegerType()),\n",
        "            StructField('actual_amount_paid', IntegerType()),\n",
        "            StructField('is_auto_renew', IntegerType()),\n",
        "            StructField('transaction_date', DateType()),\n",
        "            StructField('membership_expire_date', DateType()),\n",
        "            StructField('is_cancel', IntegerType()),\n",
        "            ])\n",
        "#read data from csv\n",
        "transactions = (\n",
        "                spark\n",
        "                  .read\n",
        "                  .csv(\n",
        "                      'gdrive/My Drive/Databricks/transactions.csv',\n",
        "                       schema=transaction_schema,\n",
        "                       header=True,\n",
        "                       dateFormat = 'yyyyMMdd'\n",
        "                  )\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obdLNRb7ESHD"
      },
      "source": [
        "# Work with the data using DELTA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-m1HTpsJo28"
      },
      "source": [
        "#persist in delta lake format - essentially a parquet file with additional features such as history and versioning\n",
        "# This creates  folders called  G:\\My Drive\\Databricks\\transactions\\transaction_date=2015-01-01 ......\n",
        "(\n",
        "  transactions\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .partitionBy('transaction_date')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/transactions')    \n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrBLeRUuesWB",
        "outputId": "6e846cf0-588e-49a5-dd3c-f6669171511d"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "    DROP  TABLE  IF EXISTS kkbox.transactions\n",
        "\"\"\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkc125ybRbJh",
        "outputId": "a374912e-60fc-4a0d-d282-d59ca563b2b0"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE  kkbox.transactions \n",
        "    USING DELTA\n",
        "    LOCATION 'gdrive/My Drive/Databricks/kkbox/transactions'\n",
        "\"\"\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkzjnOzLULkz"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.transactions LIMIT 10\n",
        "''')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ8Urc8FcVVd",
        "outputId": "a424a887-074c-44dc-9523-65e6d2064a5f"
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "|                msno|payment_method_id|payment_plan_days|plan_list_price|actual_amount_paid|is_auto_renew|transaction_date|membership_expire_date|is_cancel|\n",
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "|+2HVGotjiE2ofWgVp...|               41|               30|             99|                99|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+5Yo2rxxC+x+kIYl4...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+BForXQeVUWKHbTM/...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+L/XrtIg0DD9ku+ik...|               33|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+NFlZlsTdfUWAxDiE...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+OQ8X0rnnrPOUvtAB...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+SzJCXTBfFx9+tps9...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-12|        0|\n",
            "|+ZLD2EVyD7TQs3gUw...|               39|               30|            149|               149|            1|      2017-03-31|            2017-05-16|        0|\n",
            "|+bnLGdIZW5uxgyUA2...|               34|               30|            149|               149|            1|      2017-03-31|            2017-04-30|        0|\n",
            "|+hE9XWSj68wdPEaE8...|               36|               30|            180|               180|            1|      2017-03-31|            2017-04-30|        0|\n",
            "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfsYOip4U1xL"
      },
      "source": [
        "\n",
        "# members dataset schema\n",
        "member_schema = StructType([\n",
        "  StructField('msno', StringType()),\n",
        "  StructField('city', IntegerType()),\n",
        "  StructField('bd', IntegerType()),\n",
        "  StructField('gender', StringType()),\n",
        "  StructField('registered_via', IntegerType()),\n",
        "  StructField('registration_init_time', DateType())\n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "members = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'gdrive/My Drive/Databricks/members.csv',\n",
        "      schema=member_schema,\n",
        "      header=True,\n",
        "      dateFormat='yyyyMMdd'\n",
        "      )\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdoDbzWgYsmr"
      },
      "source": [
        "# persist in delta lake format\n",
        "(\n",
        "  members\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/members')\n",
        "  )\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM2FUTE7YtvB"
      },
      "source": [
        "\n",
        "  # create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "    CREATE TABLE kkbox.members \n",
        "    USING DELTA \n",
        "    LOCATION 'gdrive/My Drive/Databricks/kkbox/members'\n",
        "    ''')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNLrTKO0VlAs"
      },
      "source": [
        "\n",
        "#Load User Logs Table\n",
        "\n",
        "_ = spark.sql('DROP TABLE IF EXISTS kkbox.user_logs')\n",
        "\n",
        "# drop any old delta lake files that might have been created\n",
        "#shutil.rmtree('gdrive/My Drive/Databricks/kkbox/user_logs', ignore_errors=True)\n",
        "\n",
        "# user logs dataset schema\n",
        "user_logs_schema = StructType([ \n",
        "  StructField('msno', StringType()),\n",
        "  StructField('date', DateType()),\n",
        "  StructField('num_25', IntegerType()),\n",
        "  StructField('num_50', IntegerType()),\n",
        "  StructField('num_75', IntegerType()),\n",
        "  StructField('num_985', IntegerType()),\n",
        "  StructField('num_100', IntegerType()),\n",
        "  StructField('num_uniq', IntegerType()),\n",
        "  StructField('total_secs', FloatType())  \n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "user_logs = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'gdrive/My Drive/Databricks/user_logs.csv',\n",
        "      schema=user_logs_schema,\n",
        "      header=True,\n",
        "      dateFormat='yyyyMMdd'\n",
        "      )\n",
        "    )\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c09ghGwVZi80"
      },
      "source": [
        "\n",
        "# persist in delta lake format\n",
        "( user_logs\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .partitionBy('date')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/user_logs')\n",
        "  )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUPz3g7RZl0h"
      },
      "source": [
        "\n",
        "\n",
        "# create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE IF NOT EXISTS kkbox.user_logs\n",
        "  USING DELTA \n",
        "  LOCATION 'gdrive/My Drive/Databricks/kkbox/user_logs'\n",
        "  ''')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFWgyddPbDRC"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.members LIMIT 10\n",
        "''')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPSSy4GjbtAq",
        "outputId": "a748d73f-1a35-43f5-996b-a0fbc748a4fb"
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "|                msno|city| bd|gender|registered_via|registration_init_time|\n",
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "|Rb9UwLQTrxzBVwCB6...|   1|  0|  null|            11|            2011-09-11|\n",
            "|+tJonkh+O1CA796Fm...|   1|  0|  null|             7|            2011-09-14|\n",
            "|cV358ssn7a0f7jZOw...|   1|  0|  null|            11|            2011-09-15|\n",
            "|9bzDeJP6sQodK73K5...|   1|  0|  null|            11|            2011-09-15|\n",
            "|WFLY3s7z4EZsieHCt...|   6| 32|female|             9|            2011-09-15|\n",
            "|yLkV2gbZ4GLFwqTOX...|   4| 30|  male|             9|            2011-09-16|\n",
            "|jNCGK78YkTyId3H3w...|   1|  0|  null|             7|            2011-09-16|\n",
            "|WH5Jq4mgtfUFXh2yz...|   5| 34|  male|             9|            2011-09-16|\n",
            "|tKmbR4X5VXjHmxERr...|   5| 19|  male|             9|            2011-09-17|\n",
            "|I0yFvqMoNkM8ZNHb6...|  13| 63|  male|             9|            2011-09-18|\n",
            "+--------------------+----+---+------+--------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEjakLwaFTMp"
      },
      "source": [
        "# Get Training Data from CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W3qofZtE_ir"
      },
      "source": [
        "train_schema = StructType([\n",
        "            StructField('msno', StringType()),\n",
        "            StructField('is_churn', IntegerType()),\n",
        "            ])\n",
        "#read data from csv\n",
        "train = (\n",
        "                spark\n",
        "                  .read\n",
        "                  .csv(\n",
        "                      'gdrive/My Drive/Databricks/train_v2.csv',\n",
        "                       schema=train_schema,\n",
        "                       header=True\n",
        "                  )\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-JcIrkkG5MV"
      },
      "source": [
        "(\n",
        "  train\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .save('gdrive/My Drive/Databricks/kkbox/train')    \n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL3vQFPYPEy5"
      },
      "source": [
        "\n",
        "# create table object to make delta lake queriable\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE IF NOT EXISTS kkbox.train\n",
        "  USING DELTA \n",
        "  LOCATION 'gdrive/My Drive/Databricks/kkbox/train'\n",
        "  ''')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0PFTdVkPen-"
      },
      "source": [
        "capture = spark.sql('''\n",
        "SELECT * FROM  kkbox.train LIMIT 10\n",
        "''')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0WBIvJKPabS",
        "outputId": "a420433b-beb1-4b8c-a98f-e238544e028a"
      },
      "source": [
        "capture.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+\n",
            "|                msno|is_churn|\n",
            "+--------------------+--------+\n",
            "|ugx0CjOMzazClkFzU...|       1|\n",
            "|f/NmvEzHfhINFEYZT...|       1|\n",
            "|zLo9f73nGGT1p21lt...|       1|\n",
            "|8iF/+8HY8lJKFrTc7...|       1|\n",
            "|K6fja4+jmoZ5xG6By...|       1|\n",
            "|ibIHVYBqxGwrSExE6...|       1|\n",
            "|kVmM8X4iBPCOfK/m1...|       1|\n",
            "|moRTKhKIDvb+C8ZHO...|       1|\n",
            "|dW/tPZMDh2Oz/ksdu...|       1|\n",
            "|otEcMhAX3mU4gumUS...|       1|\n",
            "+--------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8zi4I_0Pl7O",
        "outputId": "08f6d420-a9f9-4849-f613-b945addf43d5"
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(msno='ugx0CjOMzazClkFzU2xasmDZaoIqOUAZPsH1q0teWCg=', is_churn=1),\n",
              " Row(msno='f/NmvEzHfhINFEYZTR05prUdr+E+3+oewvweYz9cCQE=', is_churn=1),\n",
              " Row(msno='zLo9f73nGGT1p21ltZC3ChiRnAVvgibMyazbCxvWPcg=', is_churn=1),\n",
              " Row(msno='8iF/+8HY8lJKFrTc7iR9ZYGCG2Ecrogbc2Vy5YhsfhQ=', is_churn=1),\n",
              " Row(msno='K6fja4+jmoZ5xG6BypqX80Uw/XKpMgrEMdG2edFOxnA=', is_churn=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs6DBNA5QOGA"
      },
      "source": [
        "# Set up SQL magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "3Tc3btEzQVVc",
        "outputId": "412d1f35-3465-451f-cb9c-06333fd5584c"
      },
      "source": [
        "!pip install sparksql-magic"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparksql-magic\n",
            "  Downloading sparksql_magic-0.0.3-py36-none-any.whl (4.3 kB)\n",
            "Collecting pyspark>=2.3.0\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 33 kB/s \n",
            "\u001b[?25hCollecting ipython>=7.4.0\n",
            "  Downloading ipython-7.29.0-py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.1.3)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.22-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.18.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (5.1.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.4.0->sparksql-magic) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.4.0->sparksql-magic) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.4.0->sparksql-magic) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.4.0->sparksql-magic) (0.2.5)\n",
            "Collecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=3e8af06fb1b31a3210d4df7d63c5ddd54ec5cfd41d3a63476021c07f2a8a198c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, prompt-toolkit, pyspark, ipython, sparksql-magic\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.22 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.29.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.29.0 prompt-toolkit-3.0.22 py4j-0.10.9.2 pyspark-3.2.0 sparksql-magic-0.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit",
                  "py4j",
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kesdkjq5RiQz"
      },
      "source": [
        "%load_ext sparksql_magic\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MbIBmKIR__S",
        "outputId": "911807f1-d785-498c-ca21-568c7a5796a3"
      },
      "source": [
        "%config SparkSql.max_num_rows = 20"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Config option `max_num_rows` not recognized by `SparkSql`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "D7NzJYLNS13H",
        "outputId": "658a1018-506f-4dd6-89c1-d1cc24c11768"
      },
      "source": [
        "%%sparksql\n",
        "select * from kkbox.transactions limit 20"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">msno</td><td style=\"font-weight: bold\">payment_method_id</td><td style=\"font-weight: bold\">payment_plan_days</td><td style=\"font-weight: bold\">plan_list_price</td><td style=\"font-weight: bold\">actual_amount_paid</td><td style=\"font-weight: bold\">is_auto_renew</td><td style=\"font-weight: bold\">transaction_date</td><td style=\"font-weight: bold\">membership_expire_date</td><td style=\"font-weight: bold\">is_cancel</td></tr><tr><td>+2HVGotjiE2ofWgVp37vLKGzJgBiUkdKRiBlf/Wji90=</td><td>41</td><td>30</td><td>99</td><td>99</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+5Yo2rxxC+x+kIYl4to0D1GTrKacKcaGG0sV38NLIn4=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+BForXQeVUWKHbTM/nXSRGfhhzcWmfNfvz+HbE77Pjc=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+L/XrtIg0DD9ku+ik959EtrnRQfLyf/P6GD1Z8HbkP4=</td><td>33</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+NFlZlsTdfUWAxDiEZ08D/ux0o+uY3ps1+6wy4I9ybg=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+OQ8X0rnnrPOUvtABzE+MRE9zqSjTzSrI42qawMQ2uU=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+SzJCXTBfFx9+tps9joOuArGrOsGoCD7Utj9aon7X+M=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-12</td><td>0</td></tr><tr><td>+ZLD2EVyD7TQs3gUwTUeWDce5ZxUOSNpAXFd0roNO9o=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+bnLGdIZW5uxgyUA26TXwDMnTHTuDlbj4mFEAQtmmDo=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+hE9XWSj68wdPEaE8nyb4RN7i2xRRcN0VG13OUEJ4PE=</td><td>36</td><td>30</td><td>180</td><td>180</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+xhvvfof24+SVClyP4n9YqWwvCmtj+ODhW14gOlUldc=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+yY4OSgYRiBwxCk/9jyfRkvwnjY8zpaC9TBgIpx1WFk=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-15</td><td>0</td></tr><tr><td>/3T/9RetFt4UGlip/yfz8W8IR1PsleTWMMqfDRniYtA=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-11</td><td>0</td></tr><tr><td>/Avu5fPm7P+I1VeASu4RrQHS7+aJ15MAfEs9gXR+OnI=</td><td>33</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>/BndJt9YSOh1kzEykXyHrQZKl943rqFrzR9efW2b7wE=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-24</td><td>0</td></tr><tr><td>/CQ/QEofoZl1RDyTxTiiqEPkyndOB8jmrVcfPWl3fSM=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-12</td><td>0</td></tr><tr><td>/JM8FrTRFo1Y7Q1m9s7y+xxonqKFH2q2XzeISo9AYys=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-27</td><td>0</td></tr><tr><td>/KugT/YLwkcVVjiY3TFPA7h5lRBcpXTYmzeSsKDpj1U=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-01</td><td>0</td></tr><tr><td>/M85NXODpGq7PPQM7QcsI4OzGryJO+E/YoKCbW5f5VE=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-25</td><td>0</td></tr><tr><td>/Nz6zkgxuH21icQt19QXmTBQZ+4RdN8omN6Iijbuh5M=</td><td>41</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "-k9VyMJfTQkB",
        "outputId": "81aad079-7e4d-4375-c879-20213f3102e6"
      },
      "source": [
        "%%sparksql\n",
        "select * from kkbox.transactions "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "only showing top 20 row(s)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">msno</td><td style=\"font-weight: bold\">payment_method_id</td><td style=\"font-weight: bold\">payment_plan_days</td><td style=\"font-weight: bold\">plan_list_price</td><td style=\"font-weight: bold\">actual_amount_paid</td><td style=\"font-weight: bold\">is_auto_renew</td><td style=\"font-weight: bold\">transaction_date</td><td style=\"font-weight: bold\">membership_expire_date</td><td style=\"font-weight: bold\">is_cancel</td></tr><tr><td>+2HVGotjiE2ofWgVp37vLKGzJgBiUkdKRiBlf/Wji90=</td><td>41</td><td>30</td><td>99</td><td>99</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+5Yo2rxxC+x+kIYl4to0D1GTrKacKcaGG0sV38NLIn4=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+BForXQeVUWKHbTM/nXSRGfhhzcWmfNfvz+HbE77Pjc=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+L/XrtIg0DD9ku+ik959EtrnRQfLyf/P6GD1Z8HbkP4=</td><td>33</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+NFlZlsTdfUWAxDiEZ08D/ux0o+uY3ps1+6wy4I9ybg=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+OQ8X0rnnrPOUvtABzE+MRE9zqSjTzSrI42qawMQ2uU=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+SzJCXTBfFx9+tps9joOuArGrOsGoCD7Utj9aon7X+M=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-12</td><td>0</td></tr><tr><td>+ZLD2EVyD7TQs3gUwTUeWDce5ZxUOSNpAXFd0roNO9o=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-16</td><td>0</td></tr><tr><td>+bnLGdIZW5uxgyUA26TXwDMnTHTuDlbj4mFEAQtmmDo=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+hE9XWSj68wdPEaE8nyb4RN7i2xRRcN0VG13OUEJ4PE=</td><td>36</td><td>30</td><td>180</td><td>180</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+xhvvfof24+SVClyP4n9YqWwvCmtj+ODhW14gOlUldc=</td><td>34</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>+yY4OSgYRiBwxCk/9jyfRkvwnjY8zpaC9TBgIpx1WFk=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-15</td><td>0</td></tr><tr><td>/3T/9RetFt4UGlip/yfz8W8IR1PsleTWMMqfDRniYtA=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-11</td><td>0</td></tr><tr><td>/Avu5fPm7P+I1VeASu4RrQHS7+aJ15MAfEs9gXR+OnI=</td><td>33</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr><tr><td>/BndJt9YSOh1kzEykXyHrQZKl943rqFrzR9efW2b7wE=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-24</td><td>0</td></tr><tr><td>/CQ/QEofoZl1RDyTxTiiqEPkyndOB8jmrVcfPWl3fSM=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-12</td><td>0</td></tr><tr><td>/JM8FrTRFo1Y7Q1m9s7y+xxonqKFH2q2XzeISo9AYys=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-27</td><td>0</td></tr><tr><td>/KugT/YLwkcVVjiY3TFPA7h5lRBcpXTYmzeSsKDpj1U=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-01</td><td>0</td></tr><tr><td>/M85NXODpGq7PPQM7QcsI4OzGryJO+E/YoKCbW5f5VE=</td><td>39</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-05-25</td><td>0</td></tr><tr><td>/Nz6zkgxuH21icQt19QXmTBQZ+4RdN8omN6Iijbuh5M=</td><td>41</td><td>30</td><td>149</td><td>149</td><td>1</td><td>2017-03-31</td><td>2017-04-30</td><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-W3lttqoN4E"
      },
      "source": [
        "In the churn script provided by KKBox (and used in the last step), time between transaction events is used in order to determine churn status. In situations where multiple transactions are recorded on a given date, complex logic is used to determine which transaction represents the final state of the account on that date. This logic states that when we have multiple transactions for a given subscriber on a given date, we should:\n",
        "\n",
        "Concatenate the plan_list_price, payment_plan_days, and payment_method_id values and consider the \"bigger\" of these values as preceding the others\n",
        "\n",
        "If the concatenated value (defined in the last step) is the same across records for this date, cancellations, i.e. records where is_cancel=1, should follow other transactions\n",
        "\n",
        "If there are multiple cancellations in this sequence, the record with the earliest expiration date is the last record for this transaction date\n",
        "\n",
        "If there are no cancellations but multiple non-cancellations in this sequence, the non-cancellation record with the latest expiration date is the last record on the transaction date\n",
        "\n",
        "Rewriting this logic in SQL allows us to generate a cleansed version of the transaction log with the final record for each date:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnAMolDnoKx1"
      },
      "source": [
        "select msno, transaction_date, count(1) as num_transactions_in_day\n",
        "from kkbox.transactions \n",
        "group by msno, transaction_date\n",
        "having count(1) > 4\n",
        "\n",
        "\n",
        "HRtDEo0/jjkooWgalkK98zfGf264orjfvWOlrj630OY=\t2017-03-01\t4\n",
        "\n",
        "SNlFRAsmUqnXKPofSXA8WYUc5DtmLcUMy4pXSJ3Ohz0=\t2016-02-29\t131\n",
        "\n",
        "5ty4nZkq54z93wQtBN7RHVYj8rNghBDCVBH+3xmxf0I=\t2016-01-08\t11\n",
        "\n",
        "\n",
        "With is_cancel = 1\n",
        "\n",
        "Fyu11+gQAk3rbPkPi465sp7WjjmwHQcbH6X0o13Q+jw=\t2015-09-23\t2\n",
        "\n",
        "tXjs7fNSNsKSBW5LfX8MLeEDxeGF9Advsg5Z45p1BnE=\t2015-11-21\t2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "rPQUK5zutMlL",
        "outputId": "aa0ac08a-9a26-4398-87c5-eb683708670b"
      },
      "source": [
        "%%sparksql\n",
        "select msno, transaction_date, count(1) as num_transactions_in_day\n",
        " from kkbox.transactions\n",
        " where is_cancel = 1\n",
        " group by msno, transaction_date having count(1) > 1"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "only showing top 20 row(s)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">msno</td><td style=\"font-weight: bold\">transaction_date</td><td style=\"font-weight: bold\">num_transactions_in_day</td></tr><tr><td>EGkdW+bN4VMTN4qr6FfA/qYnoLvzjn5qcoycXhJk8Gs=</td><td>2015-09-30</td><td>2</td></tr><tr><td>Bt8nwY7v+ACCZWITWPXcIXq6eyCJsB6j3/t7CyXNmWg=</td><td>2015-11-19</td><td>2</td></tr><tr><td>fEQPS8ktmKzE1I6Xqkb4NZ88iDlIWgKAMw1F9q0w+ao=</td><td>2017-03-29</td><td>2</td></tr><tr><td>u0P5vSUF3xIO/ka7RX9JT+DyTxbf5Ev+McDUCUBmzHo=</td><td>2016-07-14</td><td>2</td></tr><tr><td>F77i5M6H5v7YcJ7OCxuQPkyOs3nQF854p5raZsEPKow=</td><td>2015-07-06</td><td>2</td></tr><tr><td>lzX3vyCG7UnoZCaq3opvxWVoEn1kFwPANG52bQiXoBw=</td><td>2015-10-11</td><td>2</td></tr><tr><td>tXjs7fNSNsKSBW5LfX8MLeEDxeGF9Advsg5Z45p1BnE=</td><td>2015-11-21</td><td>2</td></tr><tr><td>IKgNdAivTI0Z0nH27dq0XRLEo9QztPZJz0O3P9xgzRA=</td><td>2017-03-26</td><td>2</td></tr><tr><td>sET8W0oMLNZuRWWx3O8RFeCdCSKxHpez/inj0Ym/i+E=</td><td>2016-12-24</td><td>2</td></tr><tr><td>OraSdEoQgxY72eIhkn5x+g7/9DnwT+/C9DEQi2oMwQU=</td><td>2015-08-31</td><td>2</td></tr><tr><td>RuB85oCHzvq5T5HDd+x/IihobSKSOMFDDrkvNdSgOYA=</td><td>2015-09-23</td><td>2</td></tr><tr><td>k+CCezSitj8Bw8pG0/Nx1yGat6TwNJxC5cQ1IZIfZss=</td><td>2015-10-07</td><td>2</td></tr><tr><td>Fyu11+gQAk3rbPkPi465sp7WjjmwHQcbH6X0o13Q+jw=</td><td>2015-09-23</td><td>2</td></tr><tr><td>3eiw9EHuTa+1brOy35Wg7BsMZ1dfbo05DGlTa2O/7ig=</td><td>2015-08-02</td><td>2</td></tr><tr><td>9U5LgnNWJW/qSN+dfH49gVAh6G7ppmHnHJ0QCtyBMmQ=</td><td>2017-03-28</td><td>2</td></tr><tr><td>PTMroO4XarK9QI4piXeyi5ycXi5XEW/zuZ+FbZuPY+Q=</td><td>2015-12-07</td><td>3</td></tr><tr><td>x4eQqb3I9MkO/OPw6Iqkjc3bOwtQpRotjxaFDTtOicw=</td><td>2015-07-09</td><td>2</td></tr><tr><td>n65p5ENsZWfCuETJNv1tsP4WhmbGuCX32dwLmS2vqo4=</td><td>2015-10-22</td><td>2</td></tr><tr><td>h54UnUPmnMEFxu4DOfMRmUkCmH5nK+3QkXRKP5cYnOY=</td><td>2017-03-21</td><td>2</td></tr><tr><td>/YW5DJ3Wlp7ZZP/0IFsXTwtK5vC6UlsokyUBHCtfhbM=</td><td>2015-08-31</td><td>2</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLaCIEvEoJvj"
      },
      "source": [
        "#where msno = '5ty4nZkq54z93wQtBN7RHVYj8rNghBDCVBH+3xmxf0I='\n",
        "#and transaction_date =  '2016-01-08'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "vTY_ML18rXlC",
        "outputId": "2923c2bc-f660-47a9-eb1e-2e0c34216178"
      },
      "source": [
        "# %%sparksql\n",
        "# SELECT distinct is_cancel from kkbox.transactions\n",
        "# is_cancel\n",
        "# 1\n",
        "# \n",
        "\n",
        "\n",
        "#      dense_rank() over (partition by msno, transaction_date order by transaction_date, plan_sort desc, is_cancel asc, membership_expire_date desc) as row_rank\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">is_cancel</td></tr><tr><td>1</td></tr><tr><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1H9mOC6Ec06"
      },
      "source": [
        "#  https://databricks.com/notebooks/churn/1-data-preparation.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "7cIOzC5ajajD",
        "outputId": "99526983-9321-4c07-9798-cb1a02ad4b21"
      },
      "source": [
        "%%sparksql\n",
        "with ps as (select       \n",
        "              msno,\n",
        "              transaction_date,\n",
        "              plan_list_price,\n",
        "              payment_plan_days,\n",
        "              payment_method_id,\n",
        "              CONCAT(CAST(plan_list_price as string), CAST(payment_plan_days as string), CAST(payment_method_id as string)) as plan_sort,\n",
        "              is_cancel,\n",
        "              membership_expire_date\n",
        "          from kkbox.transactions \n",
        "          ----where msno = 'tXjs7fNSNsKSBW5LfX8MLeEDxeGF9Advsg5Z45p1BnE='\n",
        "          ---and transaction_date =  '2015-11-21'\n",
        "          ),\n",
        "      st as( select \n",
        "              msno,\n",
        "              transaction_date,\n",
        "              plan_list_price,\n",
        "              payment_plan_days,\n",
        "              payment_method_id,\n",
        "              plan_sort,\n",
        "              is_cancel,\n",
        "              membership_expire_date,\n",
        "              RANK() OVER (PARTITION BY msno, transaction_date ORDER BY plan_sort DESC, is_cancel ASC) as sort_id \n",
        "          from ps),\n",
        "      ms as (SELECT\n",
        "              msno,\n",
        "              transaction_date, \n",
        "              MAX(sort_id) as max_sort_id\n",
        "             FROM st \n",
        "             GROUP BY msno, transaction_date),\n",
        "      s as (select \n",
        "                st.msno,\n",
        "                st.transaction_date,\n",
        "                st.plan_list_price,\n",
        "                st.payment_plan_days,\n",
        "                st.payment_method_id,\n",
        "                st.is_cancel,\n",
        "                st.membership_expire_date    \n",
        "            from st inner join ms\n",
        "              on st.sort_id = ms.max_sort_id\n",
        "              and st.msno = ms.msno\n",
        "              and st.transaction_date = ms.transaction_date),\n",
        "      --- At this point we can have multiple rows with same RANK\n",
        "      -- Now to distinguish these, we have to get the min or max expire date based on the is_cancel of the grouping\n",
        "       md as (SELECT\n",
        "                msno,\n",
        "                transaction_date,\n",
        "                plan_list_price,\n",
        "                payment_plan_days,\n",
        "                payment_method_id,\n",
        "                is_cancel,\n",
        "                 -- if is_cancel is 0 in last record then go with max membership date identified, otherwise go with lowest membership date   \n",
        "                CASE  WHEN is_cancel=0 \n",
        "                      THEN MAX(membership_expire_date)\n",
        "                      ELSE MIN(membership_expire_date) END as membership_expire_date\n",
        "              FROM s\n",
        "              GROUP BY msno, transaction_date, plan_list_price, payment_plan_days,payment_method_id,is_cancel\n",
        "            )\n",
        "select * \n",
        "from md;   \n",
        " "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "only showing top 20 row(s)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">msno</td><td style=\"font-weight: bold\">transaction_date</td><td style=\"font-weight: bold\">plan_list_price</td><td style=\"font-weight: bold\">payment_plan_days</td><td style=\"font-weight: bold\">payment_method_id</td><td style=\"font-weight: bold\">is_cancel</td><td style=\"font-weight: bold\">membership_expire_date</td></tr><tr><td>0oqPBZzM+eVU3BihtG9zNKKWyztr+H2tbXVwmwIz6Ys=</td><td>2017-02-28</td><td>149</td><td>30</td><td>39</td><td>0</td><td>2017-04-27</td></tr><tr><td>2BLnDVhNFhTHkBJxpzzC0EdkQCMy+hh2WPBxha62JHw=</td><td>2017-03-10</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-10</td></tr><tr><td>3/72p+cIQPo1vO7wW+qXj9VhDyoWFF58v4pCCBnFWsg=</td><td>2017-03-05</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-05</td></tr><tr><td>7bLxYnUVrSJTeihXKD38zyFmY9SBMS+yaSP2xvV8264=</td><td>2017-03-07</td><td>149</td><td>30</td><td>41</td><td>0</td><td>2017-04-07</td></tr><tr><td>8MGd5VIIDMUmE2nc4szB9JYwoSjI8vC7lPrBrpS8XOI=</td><td>2017-03-31</td><td>149</td><td>30</td><td>39</td><td>0</td><td>2017-06-28</td></tr><tr><td>9IAKgb+Rui8RdrOwERlqiYqM4cfx4OfXI6bcmXRcRsA=</td><td>2017-03-31</td><td>149</td><td>30</td><td>34</td><td>0</td><td>2017-04-30</td></tr><tr><td>CyA0jQ7hLTeprzdOgKrB1o36azmuTiiEAE4iQXIDnME=</td><td>2017-03-23</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-23</td></tr><tr><td>GtfCdLRDElsnRdvFLu0yMZJC0agr+DFrvkSPFDZ0Z/k=</td><td>2016-12-19</td><td>447</td><td>120</td><td>38</td><td>0</td><td>2017-04-30</td></tr><tr><td>IXV+Us084HdvC+0rEbwbvsYDAVdLIaDXPL8bJ7NKOtU=</td><td>2017-03-02</td><td>99</td><td>30</td><td>41</td><td>1</td><td>2017-03-02</td></tr><tr><td>K3QPAGM9Y8nysy6CrqQX1b0xgM57M/jInMSdJeOBi28=</td><td>2017-03-14</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-14</td></tr><tr><td>L/I0exUUX8Uiix/sO6+uBgV0u3ddkH9D6ZOsxyoMD80=</td><td>2017-03-22</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-22</td></tr><tr><td>M3PpoknDXzxOhlXSfBvc/h44jB/WVetwh4bvwMkgf7s=</td><td>2017-03-02</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-02</td></tr><tr><td>MlaOhrfa/JS6xEavcFHc6PRJ4cvRIiEPu3KfhPmHmp8=</td><td>2017-03-20</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-20</td></tr><tr><td>OS3+GV3hCS++SBnO/y6grqawH0yAldx3mYx4uA2guyM=</td><td>2017-03-10</td><td>149</td><td>30</td><td>41</td><td>0</td><td>2017-04-10</td></tr><tr><td>arDxq4qag6Amu99LlQNxNi13sc78N9nZEMhYC3YH8WE=</td><td>2017-03-31</td><td>149</td><td>30</td><td>33</td><td>0</td><td>2017-05-28</td></tr><tr><td>cgNoNZtp3w7BCPJx1KzzWrbaidstVp/M+vOEh7iC4o4=</td><td>2017-03-20</td><td>149</td><td>30</td><td>41</td><td>0</td><td>2017-04-20</td></tr><tr><td>doEvrKb7VgmJJHU9cK2I4g3F2nSv30xCIjwJLuo8eHs=</td><td>2017-03-27</td><td>149</td><td>30</td><td>41</td><td>0</td><td>2017-04-27</td></tr><tr><td>dp0phiHDZl3cFkaRLup+FaqWKKQyrrQ56nNx03Vy4tQ=</td><td>2017-03-16</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-16</td></tr><tr><td>e4T+b+KYjxLvs3o7teJvb499QfJwRStIJVQPLeX0s5k=</td><td>2017-03-07</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-07</td></tr><tr><td>em6uE4Rf14q/QVkfQYKOXSqBLNrJWE3dwiitCJZzJpY=</td><td>2017-03-23</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-04-23</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        },
        "id": "h1VgcupC0NPr",
        "outputId": "b0d4c586-5f57-434b-e048-32f2bc686675"
      },
      "source": [
        "%%sparksql\n",
        "with ps as (select       \n",
        "              msno,\n",
        "              transaction_date,\n",
        "              plan_list_price,\n",
        "              payment_plan_days,\n",
        "              payment_method_id,\n",
        "              CONCAT(CAST(plan_list_price as string), CAST(payment_plan_days as string), CAST(payment_method_id as string)) as plan_sort,\n",
        "              is_cancel,\n",
        "              membership_expire_date\n",
        "          from kkbox.transactions \n",
        "          where msno = 'tXjs7fNSNsKSBW5LfX8MLeEDxeGF9Advsg5Z45p1BnE='\n",
        "          and transaction_date =  '2015-11-21'\n",
        "          ),\n",
        "      st as( select \n",
        "              msno,\n",
        "              transaction_date,\n",
        "              plan_list_price,\n",
        "              payment_plan_days,\n",
        "              payment_method_id,\n",
        "              plan_sort,\n",
        "              is_cancel,\n",
        "              membership_expire_date,\n",
        "              DENSE_RANK() OVER (PARTITION BY msno, transaction_date ORDER BY plan_sort DESC, is_cancel ASC) as sort_id \n",
        "          from ps)\n",
        "select        msno,\n",
        "              transaction_date,\n",
        "              plan_list_price,\n",
        "              payment_plan_days,\n",
        "              payment_method_id,\n",
        "              is_cancel,\n",
        "              membership_expire_date\n",
        "from st \n",
        "where sort_id = 1;   "
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">msno</td><td style=\"font-weight: bold\">transaction_date</td><td style=\"font-weight: bold\">plan_list_price</td><td style=\"font-weight: bold\">payment_plan_days</td><td style=\"font-weight: bold\">payment_method_id</td><td style=\"font-weight: bold\">is_cancel</td><td style=\"font-weight: bold\">membership_expire_date</td></tr><tr><td>tXjs7fNSNsKSBW5LfX8MLeEDxeGF9Advsg5Z45p1BnE=</td><td>2015-11-21</td><td>99</td><td>30</td><td>41</td><td>0</td><td>2017-05-27</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "IBiwqTFxpNzs",
        "outputId": "1f04fc29-f972-434c-f4f2-b63e7fe820eb"
      },
      "source": [
        "%%sparksql\n",
        "with t as (select msno, transaction_date, count(1) as num_transactions_in_day \n",
        "            from kkbox.transactions\n",
        "            \n",
        " )\n",
        " select * from t"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "only showing top 20 row(s)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">msno</td><td style=\"font-weight: bold\">transaction_date</td><td style=\"font-weight: bold\">num_transactions_in_day</td></tr><tr><td>VJXQdkWQcOiiq0ut+w9I9Xx8RV2BXoJLBAlcDbKsP60=</td><td>2017-03-31</td><td>2</td></tr><tr><td>HRtDEo0/jjkooWgalkK98zfGf264orjfvWOlrj630OY=</td><td>2017-03-01</td><td>4</td></tr><tr><td>mb5S71rlAHhPolzTlONjhMKcQ2RNtYKK+SbJeyLYCH4=</td><td>2017-03-01</td><td>2</td></tr><tr><td>4xEwDfqgdUUG3M8DA97mLzZ7Y+27H+Ow8ooFEdTf8Mw=</td><td>2017-03-05</td><td>2</td></tr><tr><td>LNaoMq4j7AnAxDAGjgW+pKF6QurfwYGUte9u7s5nckE=</td><td>2017-03-06</td><td>2</td></tr><tr><td>GrSKHxYbDBkoUkiblX3Fd9h19mLu2uTa227fi76xICc=</td><td>2017-03-16</td><td>2</td></tr><tr><td>9dDK1piiFcjOjyI7Lv43Riwkq2YJcroCxEjaG5q5YqA=</td><td>2017-03-16</td><td>2</td></tr><tr><td>MzOogm9ZV3d+VYT9MhYXiNwMHIYxGEpGphXkP2NZHZw=</td><td>2017-03-04</td><td>2</td></tr><tr><td>WpEdd88Bnk46ZRGDeZt7hK4IlBv8QbLnYif9M/09p7o=</td><td>2017-03-11</td><td>2</td></tr><tr><td>5tdey2pyjb7vhsC6g8V+XpLqpmIBzMX33ud53hllbzw=</td><td>2017-03-11</td><td>2</td></tr><tr><td>kLmfv73WGidzd/d07+viS/iliL+vefGI9RYX7oUPJJw=</td><td>2017-03-09</td><td>2</td></tr><tr><td>uTxzKXheSxqME3+n0GQPOb0SyN807/pUoBKhuAB+mpY=</td><td>2017-03-09</td><td>2</td></tr><tr><td>BoX/eqmRGKTo413Wn6MOy0dsyoSzd0B3bHX2X6VAbmE=</td><td>2017-03-03</td><td>2</td></tr><tr><td>AofhGl9Kzda9aAzfguHWRegTdDsYah0TZAlBMSDDVLE=</td><td>2017-03-03</td><td>2</td></tr><tr><td>Vh9kBf0AW2uE8ylukNXmqkFD8+qV9yyi2jAn5YOL150=</td><td>2017-03-03</td><td>2</td></tr><tr><td>sRCue45uQzOkYm8Zsps7aUbFcypMQB4C5S+EleJyHAw=</td><td>2017-03-18</td><td>3</td></tr><tr><td>RPusirhIAklVMVtUQkETE8dODquLlM1zAF7Xe2oGtQU=</td><td>2017-03-21</td><td>2</td></tr><tr><td>fKdBGo29riLk/i6UiJxuVJrhM1w8B4GmJWNoDBhy5wA=</td><td>2017-03-15</td><td>2</td></tr><tr><td>9kFOZNda0p1JikDoJ0WprNEJf7aVoVPoFmSixOyMoaY=</td><td>2017-03-15</td><td>2</td></tr><tr><td>3ufZgNJHKplRuM3b4y/PM2PC7dLfie1KwtYyL0vm9h4=</td><td>2017-03-19</td><td>2</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTyjsN-6FC_E"
      },
      "source": [
        "# Using Scala For labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_DJq3ljdq-y"
      },
      "source": [
        "# Step 2: Acquire Churn Labels\n",
        "# To build our model, we will need to identify which customers have churned within two periods of interest.\n",
        "# These periods are February 2017 and March 2017. We will train our model to predict churn in February 2017 and then evaluate our model's ability \n",
        "# to predict churn in March 2017, making these our training and testing datasets, respectively.\n",
        "\n",
        "# Per instructions provided in the Kaggle competition, a KKBox subscriber is not identified as churned until he or she fails to renew their \n",
        "#subscription 30-days following its expiration. Most subscriptions are themselves on a 30-day renewal schedule (though some subscriptions renew on significantly longer cycles). This means that identifying churn involves a sequential walk through the customer data, looking for renewal gaps that would indicate a customer churned on a prior expiration date.\n",
        "\n",
        "# While the competition makes available pre-labeled training and testing datasets, train.csv and train_v2.csv, respectively, several past \n",
        "#participants have noted that these datasets should be regenerated. A Scala script for doing so is provided by KKBox. Modifying the script for this environment, we might regenerate our training and test datasets as follows:"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwz43Q0t3H9"
      },
      "source": [
        "# %scala\n",
        "\n",
        "# import java.time.{LocalDate}\n",
        "# import java.time.format.DateTimeFormatter\n",
        "# import java.time.temporal.ChronoUnit\n",
        "\n",
        "# import org.apache.spark.sql.{Row, SparkSession}\n",
        "# import org.apache.spark.sql.functions._\n",
        "# import scala.collection.mutable\n",
        "\n",
        "# def calculateLastday(wrappedArray: mutable.WrappedArray[Row]) :String ={\n",
        "#   val orderedList = wrappedArray.sortWith((x:Row, y:Row) => {\n",
        "#     if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "#       x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "#     } else {\n",
        "      \n",
        "#       val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "#         x.getAs[String](\"payment_plan_days\") +\n",
        "#         x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "#         y.getAs[String](\"payment_plan_days\") +\n",
        "#         y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       //same plan, always subscribe then unsubscribe\n",
        "#       if(x_sig != y_sig) {\n",
        "#         x_sig > y_sig\n",
        "#       } else {\n",
        "#         if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "#           //multiple cancel, consecutive cancels should only put the expiration date earlier\n",
        "#           x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "#         } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "#           //multiple renewal, expiration date keeps extending\n",
        "#           x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "#         } else {\n",
        "#           //same day same plan transaction: subscription preceeds cancellation\n",
        "#           x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "#         }\n",
        "#       }\n",
        "#     }\n",
        "#   })\n",
        "#   orderedList.last.getAs[String](\"membership_expire_date\")\n",
        "# }\n",
        "\n",
        "# def calculateRenewalGap(log:mutable.WrappedArray[Row], lastExpiration: String): Int = {\n",
        "#   val orderedDates = log.sortWith((x:Row, y:Row) => {\n",
        "#     if(x.getAs[String](\"transaction_date\") != y.getAs[String](\"transaction_date\")) {\n",
        "#       x.getAs[String](\"transaction_date\") < y.getAs[String](\"transaction_date\")\n",
        "#     } else {\n",
        "      \n",
        "#       val x_sig = x.getAs[String](\"plan_list_price\") +\n",
        "#         x.getAs[String](\"payment_plan_days\") +\n",
        "#         x.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       val y_sig = y.getAs[String](\"plan_list_price\") +\n",
        "#         y.getAs[String](\"payment_plan_days\") +\n",
        "#         y.getAs[String](\"payment_method_id\")\n",
        "\n",
        "#       //same data same plan transaction, assumption: subscribe then unsubscribe\n",
        "#       if(x_sig != y_sig) {\n",
        "#         x_sig > y_sig\n",
        "#       } else {\n",
        "#         if(x.getAs[String](\"is_cancel\")== \"1\" && y.getAs[String](\"is_cancel\") == \"1\") {\n",
        "#           //multiple cancel of same plan, consecutive cancels should only put the expiration date earlier\n",
        "#           x.getAs[String](\"membership_expire_date\") > y.getAs[String](\"membership_expire_date\")\n",
        "#         } else if(x.getAs[String](\"is_cancel\")== \"0\" && y.getAs[String](\"is_cancel\") == \"0\") {\n",
        "#           //multiple renewal, expire date keep extending\n",
        "#           x.getAs[String](\"membership_expire_date\") < y.getAs[String](\"membership_expire_date\")\n",
        "#         } else {\n",
        "#           //same date cancel should follow subscription\n",
        "#           x.getAs[String](\"is_cancel\") < y.getAs[String](\"is_cancel\")\n",
        "#         }\n",
        "#       }\n",
        "#     }\n",
        "#   })\n",
        "\n",
        "#   //Search for the first subscription after expiration\n",
        "#   //If active cancel is the first action, find the gap between the cancellation and renewal\n",
        "#   val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
        "#   var lastExpireDate = LocalDate.parse(s\"${lastExpiration.substring(0,4)}-${lastExpiration.substring(4,6)}-${lastExpiration.substring(6,8)}\", formatter)\n",
        "#   var gap = 9999\n",
        "#   for(\n",
        "#     date <- orderedDates\n",
        "#     if gap == 9999\n",
        "#   ) {\n",
        "#     val transString = date.getAs[String](\"transaction_date\")\n",
        "#     val transDate = LocalDate.parse(s\"${transString.substring(0,4)}-${transString.substring(4,6)}-${transString.substring(6,8)}\", formatter)\n",
        "#     val expireString = date.getAs[String](\"membership_expire_date\")\n",
        "#     val expireDate = LocalDate.parse(s\"${expireString.substring(0,4)}-${expireString.substring(4,6)}-${expireString.substring(6,8)}\", formatter)\n",
        "#     val isCancel = date.getAs[String](\"is_cancel\")\n",
        "\n",
        "#     if(isCancel == \"1\") {\n",
        "#       if(expireDate.isBefore(lastExpireDate)) {\n",
        "#         lastExpireDate = expireDate\n",
        "#       }\n",
        "#     } else {\n",
        "#       gap = ChronoUnit.DAYS.between(lastExpireDate, transDate).toInt\n",
        "#     }\n",
        "#   }\n",
        "#   gap\n",
        "# }\n",
        "\n",
        "# val data = spark\n",
        "#   .read\n",
        "#   .option(\"header\", value = true)\n",
        "#   .csv(\"/mnt/kkbox/transactions/\")\n",
        "\n",
        "# val historyCutoff = \"20170131\"\n",
        "\n",
        "# val historyData = data.filter(col(\"transaction_date\")>=\"20170101\" and col(\"transaction_date\")<=lit(historyCutoff))\n",
        "# val futureData = data.filter(col(\"transaction_date\") > lit(historyCutoff))\n",
        "\n",
        "# val calculateLastdayUDF = udf(calculateLastday _)\n",
        "# val userExpire = historyData\n",
        "#   .groupBy(\"msno\")\n",
        "#   .agg(\n",
        "#     calculateLastdayUDF(\n",
        "#       collect_list(\n",
        "#         struct(\n",
        "#           col(\"payment_method_id\"),\n",
        "#           col(\"payment_plan_days\"),\n",
        "#           col(\"plan_list_price\"),\n",
        "#           col(\"transaction_date\"),\n",
        "#           col(\"membership_expire_date\"),\n",
        "#           col(\"is_cancel\")\n",
        "#         )\n",
        "#       )\n",
        "#     ).alias(\"last_expire\")\n",
        "#   )\n",
        "\n",
        "# val predictionCandidates = userExpire\n",
        "#   .filter(\n",
        "#     col(\"last_expire\") >= \"20170201\" and col(\"last_expire\") <= \"20170228\"\n",
        "#   )\n",
        "#   .select(\"msno\", \"last_expire\")\n",
        "\n",
        "\n",
        "# val joinedData = predictionCandidates\n",
        "#   .join(futureData,Seq(\"msno\"), \"left_outer\")\n",
        "\n",
        "# val noActivity = joinedData\n",
        "#   .filter(col(\"payment_method_id\").isNull)\n",
        "#   .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "\n",
        "# val calculateRenewalGapUDF = udf(calculateRenewalGap _)\n",
        "# val renewals = joinedData\n",
        "#   .filter(col(\"payment_method_id\").isNotNull)\n",
        "#   .groupBy(\"msno\", \"last_expire\")\n",
        "#   .agg(\n",
        "#     calculateRenewalGapUDF(\n",
        "#       collect_list(\n",
        "#         struct(\n",
        "#           col(\"payment_method_id\"),\n",
        "#           col(\"payment_plan_days\"),\n",
        "#           col(\"plan_list_price\"),\n",
        "#           col(\"transaction_date\"),\n",
        "#           col(\"membership_expire_date\"),\n",
        "#           col(\"is_cancel\")\n",
        "#         )\n",
        "#       ),\n",
        "#       col(\"last_expire\")\n",
        "#     ).alias(\"gap\")\n",
        "#   )\n",
        "\n",
        "# val validRenewals = renewals.filter(col(\"gap\") < 30)\n",
        "#   .withColumn(\"is_churn\", lit(0))\n",
        "# val lateRenewals = renewals.filter(col(\"gap\") >= 30)\n",
        "#   .withColumn(\"is_churn\", lit(1))\n",
        "\n",
        "# val resultSet = validRenewals\n",
        "#   .select(\"msno\",\"is_churn\")\n",
        "#   .union(\n",
        "#     lateRenewals\n",
        "#       .select(\"msno\",\"is_churn\")\n",
        "#       .union(\n",
        "#         noActivity.select(\"msno\",\"is_churn\")\n",
        "#       )\n",
        "#   )\n",
        "\n",
        "# resultSet.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/kkbox/silver/train/\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5E0GEgexe5x"
      },
      "source": [
        "#https://pixiedust.github.io/pixiedust/install.html"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}